\chapter{Introduction}

Poor ambient air quality was estimated to cause over 9 million deaths worldwide in 2015 and up to 25\% of all deaths in some severely affected countries \citep{Landrigan2017}. In addition to direct health impacts and enhanced mortality, air pollution is also estimated to cost world economies from \$4-6 trillion USD annually due to lost time and health care costs \citep{Landrigan2017, Narain2016}, with over \$1.57 Trillion USD lost in Europe alone \citep{Roy2015}.

Currently, over 92\% of the world's population live in areas where air quality exceeds World Health Organization (WHO) limits \citep{WHO2016} with many of these people living in dense, urban centers. Air pollutants include primary products of combustion, products of incomplete combustion, fugitive emissions, biogenic and agricultural sources, and secondary products of combustion caused by photochemical transformation. The US Environmental Protection Agency (USEPA) lists 188 hazardous air pollutants (HAPs) that generators are required to report on an annual basis \citep{USEPA1996}. The primary pollutants that most regulatory agencies monitor as part of their ambient air quality program is a subset of that list that includes sulfur dioxide (SO$_{2}$), carbon monoxide (CO), nitrogen dioxide (NO$_{2}$), tropospheric ozone (O$_{3}$),  particulate matter less than 10 microns (PM$_{10}$), and particulate matter less than 2.5 microns (PM$_{2.5}$). Each pollutant is generated in part, as a primary or secondary process, from the thermodynamic combustion of fossil fuels.

For developing nations, heavy reliance is placed on highly polluting fuel-based combustion sources to generate electricity, power industry, transport people and products, build infrastructure, and improve the quality of life of its citizens. For many countries, development investment comes from exploitation of natural resources such as hydrocarbon reserves, minerals, or agricultural products - all industries that are large consumers of fossil fuels.

Regulatory agencies in developing countries often face shortages in skilled personnel, funding and political will in implementing and enforcing environmental regulations and programs to protect air quality \citep{Freeman2015a}. Providing effective baseline studies and tools to establish air management programs are required that can prioritize the limited resources an agency has. 

While the major expenses in air management offices are the costs to install and maintain air monitoring stations,  little effort is given to using the data as a tool to forecast air quality levels and focus emission reduction efforts. Along with knowing what the current and historical air quality concentrations are, the most important information an air manager can have is to know where his major polluting sources are, what pollutants they generate, how much they generate of each pollutant, and when will the emissions generated by these sources create harmful concentrations.

The purpose of this research is to create novel air quality management (AQM) approaches that can support initiatives in developing nations. Most countries have air monitoring stations that provide ambient air quality concentrations as well as meteorological parameters. Using historical AMS data sets with open source databases available from various government agencies, stochastic modeling, and machine learning techniques, local air managers can access advanced capabilities that would typically require expensive studies and data collection efforts.

This research used the AQM system prepared for the State of Kuwait to validate and improve the novel AQM program. A series of tools were developed using machine learning and statistical models to approximate air quality zones, prepare initial national emissions inventory, and forecast air quality concentrations.  These tools were validated with local historical air quality monitoring data along with open access statistics from government sources.

\section{Thesis Outline}
This thesis is organized in a \textit{manuscript} format according to the University of Guelph 2016-2017 Graduate Calendar ``Thesis Format'' section (\url{https://www.uoguelph.ca/graduatestudies/current-students/preparation-your-thesis}).  The chapters are outlined as follows:

\begin{itemize}

\item \textbf{Chapter 1 - Introduction.} This chapter serves as an overview to the research covered by the thesis. It describes the work completed as well as provides a literature review of the different tasks.

\item \textbf{Chapter 2 - Problem Description.} This chapter describes the research objectives for each task and scopes the problem domain.

\item \textbf{Chapter 3 - Methodology.} This chapter provides the different methodologies developed to investigate the problem statements defined in Chapter 2.

\item \textbf{Chapter 4 - Analysis of Results and Validation.} This chapter presents results of the methods developed in Chapter 3 and compares them with other published results or expected outcomes to confirm their usefulness to addressing the problem statements of Chapter 2.

\item \textbf{Chapter 5 - Conclusions and Recommendations.} This final chapter emphasizes the overall contributions made by this research and provides recommendations for future work.

\end{itemize}
\section{Literature Review}

\subsection{Air zones review}
The use of air quality zones (AQZs) as a tool for air management was identified as early as the 1960s \citep{Breivogel1961, Holland1960} and was first mandated State Implementation Plans (SIPs) under the US Clean Air Act (CAA) of 1990 (USEPA, 1990). In the United States, AQZs are based on designated air quality control regions (AQCRs) that include defined municipalities and groups of intrastate and interstate counties (40CFR81, 1991). Currently, there are 264 designated AQCRs with 121 in some form of non-compliance (or non-attainment) with the NAAQS (USEPA, 2016). In general, AQZs describe areas where air pollutants are below ambient standards (“in attainment” or “in compliance”), or exceeding them (“in non-attainment” or “in non-compliance”).  The United States Environmental Protection Agency (USEPA) uses ambient air monitoring data from state regulatory agencies measured in each county to determine non-attainment areas \citep{Carr2012}.  USEPA originally initially required states to identify zones of non-attainment for 8-hour ozone.  These regulatory requirements later included sulfur dioxide, Particulate Matter less than 10 microns (PM$_{10}$), and less than 2.5 microns (PM$_{2.5}$).  A consequence of these regulations was the installation of thousands of monitoring stations throughout the USA.  In 2014, 1,293 active ozone monitoring stations were in operation at areas considered to be at risk (USEPA, 2015).  Despite the widespread placement of monitoring equipment, designation of AQZs (or AQCRZs) are limited to areas defined by county lines \citep{Carr2012}.

The European Union initiated a zone management system for its member states in 2008 with Directive 2008/50/EC (EC, 2008).  In addition to designated AQZs, the directive requires at least one monitor for PM$_{2.5}$ in every 100,000 km$^{2}$.  Low emission zones (LEZs) were established in over 200 European cities that enforced restrictions on cars and heavy-duty vehicles (Holman et al., 2015).  The drawback of LEZs is that the zones were established based on city boundaries and not by geophysical features \citep{Henschel2013}.

Turkey followed the European Directive to define AQZs for eight different regional areas on its 81 provinces \citep{CYGM2010}.  The process used multiple methods and tools, including statistical analysis of monitoring station historical data, use of geodatabases and Geographic Information System (GIS) evaluations, air dispersion and local sources, as well as meteorological effects \citep{Karaca2012}.  Furthermore, the Turkish evaluation only employed average daily values of pollutants, instead of hourly averages, with daily values discarded if more than 30\% of hourly data was unavailable.  

China established Control Zones in 1995 that covered over 11.4\% of its territory to manage ever-increasing levels of SO$_{2}$ \citep{Hao2000}.  Additional acid rain control zones covered almost 8.4\% of China.  Zones were established based on local meteorological, topographical and soil conditions that had low pH (acidic).  Other designation factors included precipitation pH values and ambient SO$_{2}$ measurements.

\subsubsection{Coastal Urban Centers}

Approximately 40\% of the population of the United States (over 100 million people) lives in coastal shoreline counties \citep{NOAA2013}.  An estimate of the world population living in coastal areas was approximately 1.4 billion people in 2015, with expected populations growing to over 1.6 billion by 2024 \citep{Geohive2015}.  With the growth of coastal communities comes air management challenges \citep{Gamas2015}.  Local ambient air quality is impacted by the volume of emissions from industry, transportation, and residential activities, as well as meteorological and seasonal changes \citep{Fiore2015, Kimbrough2013}.  Coastal zones are also subject to land-sea breezes, caused by the diurnal differential heating/cooling of the sea and land \citep{Crosman2010, Cuxart2014, Tsai2011}.  Land-sea breezes (LSBs) continuously shift direction and speed up over the course of the day, sometimes extending as deep as 150 km inland depending on topography, and even deeper for desert regions \citep{Miao2015, Zhu2004}.  Variables that impact the extent and intensity of LSBs are largely geophysical and include surface heat flux, wind patterns, atmospheric stability and moisture, dimensions of local water bodies, shoreline curvature, topography, and surface roughness \citep{Crosman2010, Lu1995}.  These shifting winds recirculate emissions, which have negative impacts on coastal communities \citep{Lu1996}.  Modeling and evaluating the impact of this recirculation has been the subject of several studies. \citep{Crosman2010, Levy2009, Wu2013, Zhu2004}.  A key objective of this study was to determine the extent of LSBs along a developed coastline using a quantitative method that incorporated multi-year weather patterns. 

Many researchers have applied statistical approaches to evaluating LSB.  Levy et al. (2009) developed a spatiotemporal model to evaluate mesoscale recirculation using a Regional Atmospheric Modeling System HYbrid Particle and Concentration Transport (RAMS-HYPACT) modeling system and inert particles.  They used the recirculation potential index ($R$) to measure recirculation potential at each grid cell.  The $R$ index is a ratio of the wind’s vector and scalar sums over a 24 hour period.  A low numerical $R$ index indicates a continually changing wind direction, while a high value indicates constant wind direction during the time.  Existing sources were used for model inputs, and results compared to readings from over 26 air monitoring station distributed throughout the area of evaluation.  

Wu et al. (2013) also used an $R$ index to identify LSB impacts as well as a Ventilation Index (VI) based on wind speeds at different altitudes.  Both Levy and Wu required measurements at different heights using radio soundings or balloons to capture vertical wind speeds.  Neither team’s findings were incorporated into air zone management.  In addition to Levy and Wu, several studies used neural networks and other machine learning-based techniques to analyze complex dispersion patterns in coastal areas \citep{Elangasinghe2014, Feng2015}, but these methods require large data sets for training. 

\subsubsection{Classification using higher order moments}
Classifying categories based on higher order moments, such as Skewness ($S$) and Kurtosis ($K$) statistics, were applied by Martins as early as 1965 to discriminate between beach and dune sand grains \citep{Martins1965}.  Other fields have more recently used $S-K$ statistics for pattern recognition \citep{Crosilla2013}, medical recovery \citep{Chi2008} and music genre classification (Seo and Lee, 2011).  Skewness is a 3rd order moment around the sample mean that measures the symmetry (or asymmetry) of the sample’s distribution. A normal distribution has an $S$ = 0. Kurtosis is a 4th order moment that measures the peakedness of a distribution relative to a normal distribution. A normal distribution has a $K$ = 3. A highly skewed distribution with a high kurtosis will have a sharp peak and a long tail on one side of the mean \citep{NIST2013}.  The general equations for $S$ are

\begin{equation}
\label{eq:skewness}
S = \frac{\sum_{i=1}^{N}\left (x_{i}-\bar{x} \right )^{3}}{N\sigma^{3}}
\end{equation}

\noindent
and for $K$ is 

\begin{equation}
\label{eq:kurtosis}
K = \frac{\sum_{i=1}^{N}\left (x_{i}-\bar{x} \right )^{4}}{N\sigma^{4}}
\end{equation}

\noindent
where $x$ is an individual data point, $\bar{x}$ is the data mean, $\sigma$ is the standard deviation, and $N$ is the number of data points \citep{Cristelli2012}.    Some Kurtosis formulas apply a correction term of ``-3" to Eq \ref{eq:kurtosis} (called excess Kurtosis) to bias the equation to obtain K = 0 for a normal distribution.  Kurtosis is also calculated as shown in eq \ref{eq:xskurtosis}.

\begin{equation}
\label{eq:xskurtosis}
K = \frac{N(N+1)}{(N-1)(N-2)(N-3)} \frac{\sum_{i=1}^{N}\left (x_{i}-\bar{x} \right )^{4}}{\sigma^{4}}
\end{equation}

The first term, $(N(N+1))/((N-1)(N-2)(N-3))$, is approximately 1/N for large values of N (N$>$200), thus reducing eq \ref{eq:xskurtosis} down to eq \ref{eq:kurtosis} \citep{Cox2010}.  Differences between distributions with various $S$ and $K$ values are shown below in Figure \ref{fig:SKcurves}.  The blue, solid line curve shows a normal distribution with $S$ = 0 and $K$ = 3. The green dot-dash line shows a Weibull distribution with $S$ = 1.7 and $K$ = 7.4.  The red dash line shows a log-normal distribution with $S$ = 4 and $K$ = 41. 
%
\begin{figure}
\includegraphics[width=\linewidth,keepaspectratio]{images/aqz1.png} 
\caption{Distributions representing various $S$ and $K$ statistics.}
\label{fig:SKcurves}
\end{figure}
%
Using the higher order moments amplifies the error of the sample points from the sample mean as well as its variance to allow better classification \citep{Seo2011}.  Crosilla et al. (2013) applied $S$ and $K$ statistics to remote sensor images in order to group terrain into similar categories as based on datasets collected from Light Detection and Ranging (LiDAR).  Alberghi et al. (2002) used Kurtosis to identify vertical velocity components in convective boundary layers associated with LSBs.  They used measured data and data from air dispersion models to identify correlations between Skewness and Kurtosis \citep{Alberghi2002}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% END OF SECTION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\clearpage
\subsection{Risk characterization review}

Ambient air quality standards are specified by most countries to protect human health and reduce impacts of hazardous air pollution emissions for the public welfare. The Clean Air Act of 1970 (1970 CAA) was one of the first national legislation that established national-wide concentration levels for ambient air quality.  The original 1970 CAA National Ambient Air Quality Standards (NAAQS) included six criteria air pollutants (CAPs) with both primary (human health) and secondary (public welfare such as visibility, crops, and building material) standards \citep{USEPA1970}. A standard is assumed to include a specific air pollutant, its threshold concentration value, and the averaging time associated with the concentration value. In some cases, pollutants have multiple standards such as nitrogen dioxide (NO$_{2}$) that has a 1 hr average standard (100 parts per billion ($ppb$), 98th percentile of 1-hour daily maximum concentrations, averaged over 3 years) and an annual average standard (53 $ppb $, annual mean). The current NAAQS includes seven air pollutant categories and 12 different standards. European air standards began with levels for sulfur dioxide and suspended particulates in 1980 \citep{EEC1980}. The World Health Organization (WHO) published recommended air quality guidelines (AGQs) as early as 1987 for Europe with ambient levels and averaging periods for 28 different chemicals and 39 standards \citep{Lubkert1994}. The current European air quality directive was adopted in 2008 and includes 12 different air pollutants with 15 different standards \citep{EU2008}, while a WHO AGQ update was published in 2005 with only five pollutants, but 25 standards \citep{WHO2006}. 

\subsubsection{Compliance Classification}

The averaging periods of each pollutant are not always clear in regards to when the measurement period begins and ends.  By convention, most air monitoring stations follow the USEPA required hourly average reporting that starts at the beginning of the hour and ends on the 59th minute \citep{CAA2007}.  A similar convention holds for 24-hour averages whereby averaging starts at midnight (00:00 hrs) to 23:59 hrs. Annual averages are based on calendar years, beginning on 1 January at 00:00 hrs and end on 31 December at 23:59 hrs.  In the case of particulate matter, reporting follows set 24 hour periods.  The USEPA’s method for taking three-year averages is to average readings taken over individual quarters, average quarterly and then average the quarters over three years \citep{Cohen1999}. International standards do not have this same level of detailed description for averaging and it is the authors’ experience that USEPA methodology is assumed when guidance or interpretation is not available.

What is less clear is how results from multiple monitoring stations in one air quality zone should be aggregated. In the United States, air quality zones are based on designated air quality control regions (AQCRs) that include municipalities and groups of intrastate and interstate counties (40CFR81, 1991). Currently, there are 264 designated AQCRs with 121 in some form of non-compliance (or non-attainment) with the NAAQS \citep{USEPA2016a}. 

Air monitoring stations (AMSs) are used to measure ambient air and weather conditions to determine if an air quality zone is in or out of compliance with the local standards. The USEPA uses a ``winner-take-all” approach for some pollutants such as O$_{3}$ and PM$_{2.5}$ in that if one of the state and local air monitoring stations (SLAMS) within an AQCR registers exceedances that surpass a NAAQS standard, the entire zone is non-compliant \citep{USEPA2005a}.  Recently the USEPA has considered a more tailored approach to O$_{3}$ non-attainment designations by allowing regional offices to work with states and native American tribes to determine non-attainment areas based on local conditions and in some cases, declare only part of an AQCR a non-attainment area \citep{McCabe2015}. We will assume for the time being that the current ``winner takes all” approach is still in effect and used by air managers in order to demonstrate our procedure.

In the California South Coast air basin, there are at least 43 active monitoring stations \citep{CARB2013} to cover 10,743 square miles (27,824 sq km) and protect approximately 17 million people \citep{AQMD2010}.  If only one station measures an exceedance of carbon monoxide (CO) over the standard the entire basin is non-compliant and not just the immediate area.  An air monitor is assumed to represent the air quality of the area around it uniformly.  Murray and Newman (2014) recommend that lowest reading in a network area be used for multiple stations \citep{Murray2014}. This area could be as small as several hundred square meters in a micro-scale range of up to 100 meter radius from the station, to a middle range (up to 0.5 kilometer radius), neighborhood range (4 kilometer radius), urban range (50 kilometer radius) or regional range (several hundred kilometer radius) \citep{Pan2009}.  Siting air monitoring stations is an important process that takes many local variables into account \citep{Bermudez2010}, but may not represent an entire region.

In 2012, the State of Kuwait issued the Kuwait Ambient Air Quality Standards (KAAQS) in order to update their air management program as part of the Kuwait Integrated Environmental Management System (KIEMS) sponsored by the United Nations Development Program (UNDP) with the Kuwait Environment Public Authority (KEPA). The new standards included a ``winner-take-all” approach where an air zone was out of compliance for an air pollutant if any one AMS observed more than 3 observations (``3-strikes”) above the relevant Guideline Value (GV) in a calendar year \citep{KEPA2017}. The KAAQS are shown in \ref{tb:1kaaqs}. The GVs are managed based on hourly measurements from AMSs with different averaging periods, but no daily maximums.
%
\begin{table}[H]
\centering
\caption{Kuwait Ambient Air Quality Standards}
\label{tb:1kaaqs}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Pollutant} & \textbf{Guideline Value} & \textbf{Averaging Time} \\ \midrule
Carbon monoxide (CO) & 35 $ppm$ & 1 hour \\
Nitrogen dioxide (NO$_{2}$) & 100 $ppb$ & 1 hour \\
 & 21 $ppb$ & Annual \\
Sulfur dioxide (SO$_{2}$) & 19 $ppb$ & 24 hours \\
 & 75 $ppb$ & 1 hour \\
Ozone (O$_{3}$) & 70 $ppb$ & 8 hours \\
Lead (Pb) & 0.15 $\mu g/m^{3}$ & Quarterly \\
Particulate Material $<10$ microns (PM$_{10}$) & 350 $\mu g/m^{3}$ & 24 hours \\
Particulate Material $<2.5$ microns (PM$_{2.5}$) & 75 $\mu g/m^{3}$ & 24 hours \\ \bottomrule
\end{tabular}
\end{table}
%
The 3-Strike method was immediately considered too conservative and ambitious for a country with a relatively new air management program and a young environmental regulatory agency. While the program intended to protect human health and welfare through robust air quality standards that industry and stakeholders would strive to meet, it was also considered too stringent in that it did not take into account annual impacts of industrial output and complex land-sea breeze (LSB) weather patterns. Another compliance classification method was considered whereby three years of air monitoring data was collected to account for industrial output variations and seasonal weather effects. With this method, if 99\% of all hourly observations over the three years was less than or equal to the Guidance Value ($\leq$GV), then the zone was in compliance. If a zone has 2 or more AMSs, the measurements from individual stations would be pooled and the composite measurements evaluated to determine if they were $\leq$GV. This method was called the ``99\% Rule". Using a percentile approach is similar to existing methods used by the USEPA for 1 hr SO$_{2}$ and 1 hr NO$_{2}$, except the 99th percentile is used instead of the 98th \citep{USEPA2016a}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% END OF SECTION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Dispersed area sources review}

\subsubsection{Air Emissions Inventories}

Air emission inventories are used to identify air pollutants by source and process type over a given period (usually annually) and within assigned areas.  Many countries require an annual inventory of designated air pollutants such as the United States \citep{USEPA2014} and Kuwait \citep{KEPA2017}.  The United Nations requires signatories of the Kyoto Protocol to complete annual inventories of GHGs \citep{Paciornik2006}.  Because of the scale of emission inventories, actual annual emissions for individual sources are not available due to the lack of individual stack monitoring. In order to estimate emissions, parametric equations are developed that include factors based on the amount of fuel or feedstock consumed over the reporting period of a particular process.  An equation is developed for each pollutant from each fuel type and each process.  The quality of the calculated pollutant emission factors varies significantly depending on the degree of research applied to the fuel and process.  The most common source of published emission factors is the USEPA’s AP-42 series that includes over 27,200 different factors for pollutants from various industrial, commercial and residential processes \citep{USEPA1995}.

Different types of inventory methods exist based on the level of input data available. The Intergovernmental Panel on Climate Change (IPCC) recognizes three types of inventories.  Tier 1 inventories use IPCC published emission factors and nationally available fuel data to estimate individual pollutants.  Tier 2 uses country-specific emission factors, while Tier 3 inventories use process-specific models \citep{Paciornik2006}.  The USEPA’s AP-42 only provides a factor for ammonia (NH$_{3}$) under cigarette smoking and no factors for charcoal combustion.  IPCC Tier 1 emission factors do not include emission factor specific to tobacco product use but do include factors for GHG emissions from charcoal manufacturing and usage. 

While several studies looked at indoor air quality in rooms with active $nargyla$ pipes and smokers \citep{Fromme2009, Moon2015, Mulla2015}, specific emissions factors for $nargyla$ smoking suitable for emission inventories were not discovered during the literature review. These studies focused on measuring chemical concentrations as compared to rates of emission. Emission rate studies on individual elements of the smoking process, such as charcoal burning and tobacco burning were identified and used to create composite factors as described in the next sections. 

\subsubsection{Emissions from $nargyla$ Pipes}
The popularity of $nargyla$ pipes throughout the world has grown in popularity over the last decade \citep{Chaouachi2009, Monzer2008}, especially among young people, where over 30\% of university students in some countries reported regular use \citep{Eissenberg2009}. This increase is a concern for public health organizations and has led to many studies looking at toxic chemical exposure from smoke generated by $nargyla$ pipes \citep{Daher2010, Eissenberg2009, Monzer2008, Sepetdjian2010, Shihadeh2005}.  Chemical analysis has shown that $nargyla$ pipe smoke has higher levels of carbon monoxide (CO), polycyclic aromatic hydrocarbons (PAHs), benzene, and other Non-Methane Organic Compounds (NMOCs) than smoking cigarettes during the same period.

The basic $nargyla$ water pipe, as shown in Fig \ref{figng1:pipe} consists of a bowl containing sweetened tobacco often referred to as $sheesha$ or $ma’saal$.  The $sheesha$ comes in many flavors and does not have the same lingering odors as other tobacco products, such as cigarettes and cigars.  The tobacco is separated by a small air gap from the glowing charcoal by aluminum foil.  The base is connected to a long tube that ends in a water-filled base.  A hose attached to the base, but above the water level, allows the user to draw hot air from the charcoal onto the $sheesha$, enabling it to singe and smoke.  The smoky air is forced through the water that cools the smoke and allows the user to inhale it.  Because the $sheesha$ is not directly burned, it can be smoked for extended periods of time, sometimes over 1 hour per serving.  Studies have shown that the $sheesha$ itself contributes relatively little to the smoke other than the flavor and particulate matter with the primary source of emissions (over 95\%) coming from the combustion of the charcoal \citep{Sepetdjian2010}.

%
\begin{figure}[H]
\includegraphics[width=\linewidth,keepaspectratio]{images/ng1.png} 
\caption{$Nargyla$ pipe with labeled components.}
\label{figng1:pipe}
\end{figure}
%
While most studies on $nargyla$ smoking have focused on the health effects of exposure to the toxic gases, no studies have looked at the overall contribution of smoking to greenhouse gases (GHGs) and ambient air pollution levels.  The primary contributor of air pollutants is the combustion of charcoal used to heat the $sheesha$ tobacco, which produces the GHGs carbon dioxide (CO$_{2}$), methane (CH$_{4}$), and nitrous dioxide (N$_{2}$O) in addition to hazardous air pollutants mentioned previously.  While individual pipes contribute trace amounts of pollutants during individual smoking events, this investigation will demonstrate that the combined annual impacts within a community and country are significant.
Sheesha tobacco is a mix of cut tobacco and syrups such as molasses, honey or glycerol with fruit and spice flavors \citep{Chaouachi2009}.  Unlike other tobacco products, $sheesha$ is not burned directly. Hot air from the burning charcoal provides a heat source that singes the tobacco but does not directly burn it, creating a smoke composed of particulates and gases \citep{Daher2010} that is filtered by bubbling through water.  The water removes approximately 50\% of the particulates but does not remove the gaseous components \citep{Becquemin2008}. There is a variety of $sheesha$ smoking called saloom in which the charcoal is placed directly on the tobacco. This tobacco is different from most $sheesha$ in that it is drier and does not have added flavors or syrups. The effects of $saloom$ are not considered in this study as relatively few users opt for this method of smoking.

Emissions measured in some $nargyla$ emission exposure studies used chemically processed, fast lighting coals that are not used in the Persian Gulf countries \citep{Daher2010, Shihadeh2005}.  These types of coals have accelerants mixed with the charcoal such as potassium chlorate (CAS Number 3811-04-9) \citep{MIC2012} to allow easier lighting as compared to the traditional lump charcoal that requires an external heat source to initiate combustion.  The presence of accelerants provides an additional chemical source for emissions but also reduces the overall amount of charcoal used by a caf\'e or restaurant in that a supply of charcoal must otherwise be started and re-supplied when older coals are consumed. Studies looking at different types of $nargyla$ charcoal showed that lump charcoal had more than six times less PAH emissions than charcoals with accelerants \citep{Sepetdjian2010}.  Only lump charcoal is used in Kuwait and therefore is the focus of this study.

Charcoal is manufactured through the pyrolysis of wood until all water, and volatile compounds are removed.  The remaining mass is often 70\% pure carbon, allowing a consistent burn rate with very little smoke. Charcoal is an ideal heat source that is often used for cooking indoors and industrial heating.  Charcoal for $nargyla$ used in the Middle East is made from lemon and orange wood or grape vines in eastern Africa.  Charcoal made from coconut husks is also common for residential use.  Manufacturing charcoal is an emissions-intensive process that generates significant amounts of both HAPs and GHGs \citep{Lacaux1994}.  However, these emissions are not considered in this study.

Charcoal, once heated and glowing, burns with a surface temperature of approximately 800$^{o}$ C without additional air blowing on it \citep{Evans1977}.  Ash is formed on the surface as the combustion works its way into the fuel source. Major components of the charcoal combustion are CO$_{2}$, CO, and CH$_{4}$.  Emission factors collected from various sources are summarized in Table \ref{tb1:initialfactors}.  The factors were converted from published units (such as pounds of pollutant/ton of feedstock) to grams of pollutant/kg of feedstock (g/kg).

Only gaseous phase pollutants are quantified. Particulate matter is not considered in this study because of the assumption that the particulates stay in the room or are filtered by the ventilation system. In either case, they are assumed to not contribute to the local ambient air quality.
%
\begin{sidewaystable}  %%%%%%%%%% table rotated 90 degrees
\begin{table}[H]
\centering
\caption[Emission factors of $nargyla$ related components]{Emission factors of $nargyla$ related components in $g/kg$}
\label{tb1:initialfactors}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}rcccccccccccc@{}}
\toprule
 & \multicolumn{3}{c}{\textbf{IPCC}} & \multicolumn{5}{c}{\textbf{Literature}} & \multicolumn{4}{c}{\textbf{AP-42  (USEPA, 1995)}} \\ 
 & \multicolumn{3}{c}{\textbf{Paciornik (2006)}} & \multicolumn{2}{l}{\textbf{Akagi (2011)}} & \multicolumn{2}{l}{\textbf{Bhattacharya (2002)}} & \multicolumn{1}{l}{\textbf{Sepetdijan (2010)}} & \multicolumn{1}{l}{\textbf{Mixed}} & \multicolumn{1}{l}{\textbf{Leaves}} & \multicolumn{1}{l}{\textbf{Weeds}} & \multicolumn{1}{l}{\textbf{Cigarettes}} \\
\textbf{Compound} & \textbf{Expected} & \textbf{Lower} & \textbf{Upper} & \textbf{Expected} & \textbf{variation} & \textbf{Low} & \textbf{High} & \textbf{Expected} & \textbf{Expected} & \textbf{Expected} & \textbf{Expected} & \textbf{Expected} \\ \midrule
Carbon Dioxide (CO$_{2}$) & 3304 & 1415.5 & 7656 & 2385 &  & 2155 & 2567 &  &  &  &  &  \\
Carbon Monoxide (CO) &  &  &  & 189 & 36 & 35 & 198 &  &  & 56 & 42.5 &  \\
Methane (CH$_{4}$) & 5.9 & 1.043 & 34.8 & 5.29 & 2.42 & 6.7 & 7.8 &  &  & 6 & 1.5 &  \\
Acetylene (C$_{2}$H$_{2}$) &  &  &  & 0.42 &  &  &  &  &  &  &  &  \\
Ethylene (C$_{2}$H$_{4}$) &  &  &  & 0.44 & 0.23 &  &  &  &  &  &  &  \\
Ethane (C$_{2}$H$_{6}$) &  &  &  & 0.41 & 0.13 &  &  &  &  &  &  &  \\
Methanol (CH$_{3}$OH) &  &  &  & 1.01 &  &  &  &  &  &  &  &  \\
Formaldehyde (HCHO) &  &  &  & 0.6 &  &  &  &  &  &  &  &  \\
Acetic Acid (CH$_{3}$COOH) &  &  &  & 2.62 &  &  &  &  &  &  &  &  \\
Formic Acid (HCOOH) &  &  &  & 0.063 &  &  &  &  &  &  &  &  \\
Ammonia (NH$_{3}$) &  &  &  & 0.79 &  &  &  &  &  &  &  & 0.0009 \\
Nitrogen Oxides (NOx) &  &  &  & 1.41 &  &  &  &  & 2 &  &  &  \\
Nitrous Oxide (N$_{2}$O) & 0.118 & 0.02235 & 0.87 & 0.24 &  &  &  &  &  &  &  &  \\
NMOC &  &  &  & 11.1 &  & 6 & 10 &  &  &  &  &  \\
Total PAH &  &  &  &  &  &  &  & 0.000455 & 0.0065 &  &  &  \\
Acetaldehyde &  &  &  &  &  &  &  &  & 0.545 &  &  &  \\
VOC &  &  &  &  &  &  &  &  &  & 14 & 4.5 &  \\ \bottomrule
\end{tabular}
} %end resize
\end{table}     
\end{sidewaystable}

It is interesting to note that the IPCC factors for CO$_{2}$ are physically impossible to achieve as the maximum amount of CO$_{2}$ that could be produced from 1 kg of pure carbon is only 3,667 g. Most charcoal has a maximum total organic carbon content of 70\%, making a realistic limit of 2,567 g CO$_{2}$ of as shown in the Bhattacharya (2002) column.  This violation of mass-balance was discovered during our research and acknowledged by the IPCC.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% END OF SECTION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Vehicle density estimation}

Vehicle emissions are a significant contributor to regional air quality but one of the hardest sources to estimate due to the large number of variables associated with the calculations. While several emission inventory models exist for mobile sources, the input parameters are often difficult to get, and default values are based on inapplicable locations or out of date fleets. Traffic studies require large groups of observers to count and classify vehicles at different locations and at different times of the day or expensive intelligent traffic systems (ITSs) installed to monitor and quantify traffic statistics that can be used in models \citep{Suzuki2015}. Being able to estimate the number and type of vehicles on the road is an essential step to quantifying the overall emissions for a region. 

Signalized intersections (SIs) are important elements within road networks that provide traffic control and management. A major drawback of SIs is that they stop the continuous flow of traffic, requiring deceleration, idling, and acceleration of vehicles. This cycle of activity creates complex emissions ranging from particulate matter (PM) from brake and tire wear and hydrocarbons (HC) from incomplete combustion. Vehicle emissions generated at SIs include many more variables due to the variety of different vehicle models, their ages, maintenance histories, and how they are operated. A major influence on the amount of vehicle emissions generated is driver behavior. Aggressive driving habits, not only lead to accidents that intensify congestion but also uses more fuel for rapid accelerations and braking actions. Individual driver attitudes also contribute to stacking concentrations at SIs based on how comfortable a driver feels in approaching the vehicle in front of them. Studies have shown that stacking distances do not improve traffic flow once the signal changes to green  \citep{Ahmadi2017}. A safe distance is recommended instead, to avoid collisions.

Like stationary point sources, vehicle emission estimates are based on emission factors developed by researchers to characterize common vehicles under specific operating conditions. Most emissions factors studies used chassis dynamometers to measure direct emissions from engine exhaust but fail to capture non-exhaust emissions such as particulate matter from brake and tire wear, vapor losses from the fuel storage systems, leaks from lubricants and refrigerants, hydrocarbons entrained in dust from road surfaces, and aggressive driving \citep{Kam2012, Franco2013, Freeman2015a}. While estimating emissions for individual vehicles is not practical, aggregated emissions over time can be calculated by mobile source emission models and used for regional emissions inventories and transportation planning. For emissions at SIs, vehicle emissions can be estimated based on idling vehicles and acceleration from stationary to posted limits, as shown below

\begin{equation}
\label{eq:siemissions}
E_{SI}=\sum_{i=1}^{n}(Q_{0})_{i}t_{0} + (Q_{a})_{i}t_{a}
\end{equation}

\noindent
where $n$ is the number of vehicles in the SI queue, $Q_{0}$ is the idling emission rate for each individual vehicle in the queue, $t_{0}$,  $Q_{a}$ is the individual emission rate to accelerate up to the posted limited, and $t_{0}$   and $t_{a}$ are the queuing time and acceleration time respectively.

\subsubsection{Vehicle emission models} \label{sssec:VehEmissionModels}

One of the most widely used model in North America is the MOtor Vehicle Emission Simulator (MOVES) from the USEPA \citep{MOVES2014a}. This model uses vehicle type, road conditions, operating activities, and geography to estimate the amount of pollutants generated in a section of road. MOVES is primarily intended for mobile sources in North America. It is a complex and data-intensive model that is difficult to apply to large, regional models \citep{Zhang2011}.

Another common model is the International Vehicle Emissions (IVE) model developed by a consortium funded by the USEPA to provide a model for vehicle emissions outside of North America \citep{IVE2008}. It includes over 700 different types of vehicles, including 72 different classes of light-duty gasoline vehicles (LGDTs) with three different cylinder volume subsectors. Vehicle models are further categorized by technology classes based on vehicle exhaust controls and European standards \citep{Davis2005}. The IVE model allows for customized local traffic conditions, driving patterns, fuel quality, and vehicle types for countries outside North America \citep{Davis2010}. Results from the IVE, however, vary with underestimation of some emissions by 50\% and over-estimation of others by 350\% \citep{Hui2007}.

\subsubsection{Classical traffic estimation} \label{sssec:ClassicalTraffic}

Quantifying vehicle emissions requires, as a minimum, estimations of the number of vehicles on the road networks and the type of vehicle. In addition to these input parameters, each model uses a combination of composite mobile emissions based on average speeds, hot/cold starts, ambient temperature, prediction year and modal factors to calculate emissions\citep{Franco2013}. While the models assume constant speeds, actual driving conditions are more variable, especially in congested traffic \citep{Freeman2015b}. Vehicle emissions vary with speed with more emissions generated at higher speeds. Congestion and traffic jams, however, are the fastest growing segment of mobile source emissions, with emission rates of pollutants at slow and stop/go speeds similar to emissions at high speeds due to continuous acceleration/deceleration cycles \citep{Barth2009}. At SIs, vehicles are assumed to be stationary, although small creeping often occurs.

\subsubsection{Modern traffic estimation}

In many major urban centers, main roads are monitored by intelligent transportation systems (ITSs) that track average velocity, $v$, traffic flow, $f$, and traffic density, $\rho$ \citep{Wu2007, Abtahi2011, Bartosz2015}.

Without using expensive ITS's, different numerical models have been used to estimate vehicle density including classical statistical models \citep{Schreckenberg1995}, Kalman Filters \citep{Pourmoallem1997, Sun2004} and neural networks \citep{Ghosh-Dastidar2006}.  These models looked at how traffic flowed over time to assess traffic management strategies and required complex computations and historical data to calibrate the necessary equations for a specific stretch of road.  Monte Carlo methods have also been used to validate results of traffic flow models \citep{Mihaylova2004} but not to generate results.  These models look at traffic flow under various conditions and not at the extreme condition of congested traffic or SIs. 

\subsubsection{Unmanned Aerial Systems for traffic management} 
Unmanned aerial systems (UASs), or drones, have become widely used for non-military applications such as cartography \citep{Saadatseresht2015}, agricultural surveillance \citep{Saari2017}, environmental monitoring \citep{Capolupo2015}, utility inspection \citep{Day2017, Gomez2017} and traffic management \citep{Ahmadi2017, 2017, Salvo2017, Liu2013}. Many drones are commercially available, but can also be made from individual components and open source software \citep{Sharma2016a}.  Effective drone operations have four main components:

\begin{enumerate}
\item Aircraft. The aircraft may be fixed wing capable of straight flight or multiple rotors (MRs) that allow the aircraft to hover and rotate in flight. Fixed-wing aircraft are larger but have higher endurance than rotary wing aircraft. MR drones such as quadcopters (four rotors) are the most popular form of drones. The aircraft includes flight controls, energy storage (usually a lithium battery), a communication link with the ground station, navigational systems (typically a global positioning satellite (GPS) sensor), and a sensor suite. The suite usually consists of a camera and stabilization gimbal that allows steady shots while in flight. Other sensors may include infrared sensors, environmental sensors, or multiple cameras. Performance specifications for a DJI Phantom 3 Professional are shown in Table \ref{tb:p3p-specs}. An MR drone in flight is shown in Figure \ref{fig:p3p}.

\item Ground station. The ground station provides commands to the drone. While some software allows mission planning and semi-autonomous operations, the ground station is the link between the pilot and the drone, providing visual, position, flight status and telemetry data needed to maintain control of the aircraft. Links to the aircraft are usually through wifi with internet protocol, limiting the flight range to line of sight (approximately 5 km maximum). For longer ranges, more advanced communication protocols (and transceivers) are needed. These are normally reserved for fixed-wing drones.

\item Pilot. The remote pilot in command has ultimate control and responsibility for flight operations, just like in a manned aircraft. He (or she) is responsible for the flight worthiness of the aircraft, safety of the mission, coordination with local authorities for overflight approvals, and mission planning. In most countries, approval from local authorities is required prior to a mission in a public area. Pilots often have to be licensed by aviation agencies, although some countries exclude researchers from this requirement \citep{UAVCoach2017}.

\item Post-Processing. Once the drone has collected data, it must be processed to be useful. The data collected by the aircraft sensor suite will also include embedded metadata such as timestamps, geolocation, sensor capture azimuth, and even sensor data. This metadata, combined with the captured imagery, can be used by post-processing analysis to make different 3D models that provide accurate measurements \citep{Sona2014}. 

\end{enumerate}

\begin{table}[H]
\centering
\caption[DJI Phantom 3 Professional specifications]{DJI Phantom 3 Professional specifications (DJI, 2017).}
\label{tb:p3p-specs}
\begin{tabular}{@{}ll@{}}
\hline
\textbf{Aircraft} &  \\ \hline
Gross flying weight & 1280 g \\
Diagonal size & 350 mm \\
Hover accuracy & Vertical: +/- 0.1 m \\
 & Horizontal: +/- 1.5 m \\
Max Ceiling & 120 m AGL \\
Operating temperature & 0 - 40 deg C \\
Max flying time (100\% charge) & 26 minutes \\ \hline
\textbf{Camera} &  \\ \hline
Sensor & 1/2.3" CMOS \\
Lens & FOV 94deg 20 mm f/2.8 focus at inf. \\
ISO Range & 100-1600 \\
Max Image Size & 4000 x 3000 \\ \hline
\end{tabular}
\end{table}

%
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth,keepaspectratio]{images/p3p.png}  %assumes jpg extension
\caption{DJI Phantom 3 Professional UAS in flight.}
\label{fig:p3p}
\end{figure}
%
Drones offer a significant improvement to conventional traffic data collection systems such as ITSs and human traffic counts in that they can be deployed in many areas and capture data from different angles for optimized surveillance. Traffic cameras require secure access and a stable mount, and power access. Human counts need a safe observation location that is usually away from the traffic area or offers limited views. Capturing traffic data with a drone, either for real-time or post-mission analysis, allows flexible access to researchers \citep{Westoby2012}. 

Draw-backs to using drones includes their limited endurance, especially in commercially available drones that use a battery pack and have a typical flight time of 20-25 minutes. This translates to a realistic data collection period of 10-15 minutes, especially if there is a strong wind that the aircraft must overcome in order to return to its recovery point. Additionally, drones are limited by weather conditions and nearby obstructions. Flying is not recommended during precipitation, low visibility, winds over 22 km/h, or ambient temperature greater than 40 degrees Celsius \citep{DJI2017}.

Previous researchers using drones for traffic-related subjects focused on aerial surveillance as a way to augment ITS cameras \citep{Liu2013, Barmpounakis2016}. Angel et al. (2002) were some of the first researchers to use a drone to capture traffic densities and turn counts by using recorded video \citep{Angel2002}. More recently, Salvo et al. used drones to measure the headway of vehicles on city streets and roundabouts in Palermo using streaming video and a GPS-equipped probe vehicle that provided a reference speed \citep{Salvo2014, Salvo2017}. Ahmadhi et al. (2017) used a drone to measure traffic flow rates from different platoons stacked in various gap distances. By using a thermodynamic analogy, they showed that gap spacing negatively impacted flow rates once the signal changed \citep{Ahmadi2017}.

A simple method currently available to evaluate SI stacking is to use Structure from Motion (SfM) methods in photogrammetry. SfM uses digital elevation models (DEMs) created from point clouds within the photogrammetric model. DEMs are generated from multiple images of the same area at different angles using multi-view stereo (MVS) algorithms \citep{James2017}. Images taken by the drone have geolocation and time stamps embedded in the jpeg file that are orthorectified based on the camera parameters and stitched together to form an orthomosaic image \citep{Westoby2012}.

Take-offs and measurements can then be made on the DEM as well as providing a 3D model of the scene. This requires the traffic to be stationary during the image capture phase - making this technique ideal to quantify traffic stopped at an intersection.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% END OF SECTION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Air quality concentration prediction review}

Tropospheric ozone (O$_{3}$)is a secondary pollutant formed by complex photo-chemical processes that impacts human health, plants, and structural materials. Over 21,000 premature deaths in Europe are attributed annually to O$_{3}$ exposure \citep{WHO2008} with over 1.1 million deaths worldwide - over 20\% of all deaths attributed to respiratory diseases \citep{Malley2017}. The majority of tropospheric O$_{3}$ is generated through anthropogenic sources \citep{Lelieveld2000, Cooper2006} attributed to the photo-disassociation of NO$_{2}$ as shown below in the simplified reaction \citep{Finlayson1993}:

\begin{equation}
\label{eq:ozoneformation}
\begin{gathered}
NO_{2}+h\nu (\lambda < 430nm) \rightarrow NO+O \\
O+O_{2}\overset{M}{\rightarrow} O_{3}
\end{gathered}
\end{equation}

\noindent
where $M$ is a stabilization molecule used during the intermediate formation between O and O$_{2}$. Volatile Organic Compounds (VOCs) are not shown in Eq \ref{eq:ozoneformation}, but play a significant role in the oxidation of the primary combustion product NO to NO$_{2}$ \citep{Song2011}. In addition to nitrogen oxides (NOx's), VOCs, chlorine \citep{Thornton2010}, solar radiation (SR), relative humidity (RH) and ambient temperature also impact surface ozone formation \citep{Sadanaga2003}.  Local concentrations of O$_{3}$ are further influenced by weather patterns and terrain that disperse the pollutants, precursors, and byproducts \citep{Beck1998}. At night, O$_{3}$ reacts with NO$_{2}$ to form NO$_{3}$ (nitrate radical) \citep{Finlayson1993}:

\begin{equation}
\label{eq:nitrateformation}
O_{3} + NO_{2}\rightarrow NO_{3}+O_{2} 
\end{equation}

The NO$_{3}$ radicals react with NO$_{2}$ to form dinitrogen pentoxide (N$_{2}$O$_{5}$) which in turn forms nitric acid (HNO$_{3}$) through hydrolysis with water or aqueous particles \citep{Song2011}. The acid is finally neutralized by ammonia (NH$_{3}$) to complete the reaction chain \citep{Brown2012}.

Additional contributions to tropospheric O$_{3}$ concentrations come from the stratosphere-troposphere exchange (STE) of stratospheric ozone \citep{Tarasick2008}. The percentage of O$_{3}$ provided by STE at surface levels range from 13\% \citep{Cooper2006} to over 42\% \citep{Lelieveld2000} depending on area and conditions. With so many chemical and transport phenomena taking place throughout the day and night, modeling O$_{3}$ becomes a very complex task even before local terrain, sources and weather patterns are incorporated. Nonetheless, predicting ambient O$_{3}$ concentrations, and particularly concentrations that may exceed air quality standards is important for air managers and at-risk populations.  In cases where O$_{3}$ levels will exceed standards for long periods of time, air managers may issue air quality warnings and even limit industrial and vehicular activities \citep{Kuhlbusch2014, Welch2005}. Improving forecast accuracy provides planning and decision options that can impact receptor health and local economies.

\subsubsection{Forecasting Ozone with Machine Learning}

Due to the formation process of O$_{3}$, the actual concentration a local population is exposed to may have been generated from precursors emitted hundreds or even thousands of kilometers away \citep{Glavas2011}. Populations living in coastal regions may be exposed to pollutants generated locally but transformed and mixed with other precursors in circulating land-sea breezes \citep{Freeman2017a}. Tropospheric O$_{3}$ is, therefore, a more complex pollutant to estimate than primary pollutants such as sulfur dioxide (SO$_{2}$) or carbon monoxide (CO).

Many studies have used supervised machine learning techniques, such as artificial neural networks (ANNs) to predict O$_{3}$ time series concentrations \citep{Comrie1997, Dorling2003, Ettouney2009a, Kurt2008, Biancofiore2017}. ANNs have been shown to provide better predictive results than linear models such as Multiple Linear Regression (MLR) and time series models such as Autoregressive integrated moving averages (ARIMA) \citep{Gardner1998, Prybutok2000}. Support vector machines (SVMs) have also been applied to O$_{3}$ prediction scenarios with results that often outperform ANNs \citep{Luna2014, Papaleondias2013, Singh2013}. 

The benefits of using ANNs include not requiring \textit{a priori} assumptions of the data used for training and not requiring weighting of initial inputs \citep{Gardner1998}. In practice, dimensionality reduction is often used to remove inputs to the model that are not independent and identically distributed (IID) or offer little influence on the overall training. Principal Component Analysis (PCA) is often used to reduce the overall inputs to the model by removing transformed components but provides little variability to the actual number of features required to train \citep{Singh2013, Wang2015a}.

Most of the studies that use feed forward neural network (FFNN) architecture apply a single hidden-layer feed forward neural network architecture trained with meteorological and concentration data and have limited success in forecasting air quality. The canonical feed forward ANN model (FFNN) consists of an input layer, a hidden layer, and an output layer. Each layer is constructed from interlinked nodes that generate a value (usually between -1 and 1 or 0 and 1). The individual node model is shown in Figure \ref{fig:SingleANN}. \\
%
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,keepaspectratio]{images/single-ann.png} 
\caption{Individual node model.}
\label{fig:SingleANN}
\end{figure}
%
The node sums the weighted inputs of the previous layer, sometimes with a bias, and transforms the combined sum with a non-linear activation function, $\sigma$. The node activation equation is given by

\begin{equation}
\label{eq:perceptron}
y= \sigma(wx+b)
\end{equation}

\noindent
where $w$ is an array of weights for the connections between the previous layer and the current layer, $x$ is a vector of input values from the previous layer, and $b$ is a bias value. Common activation functions include the sigmoid, tanh, and relu functions. A general property for activation functions is that they normalize the output and have a continuous first-order derivative that can be used during the back-propagation training process \citep{Goodfellow2016}. The activation functions mentioned earlier are shown in Table \ref{tb:activations} along with their first order derivative and output range.

\begin{table}[H]
\centering
\caption{Common activation functions}
\label{tb:activations}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Name} & \textbf{Equation} & \textbf{Derivative} & \textbf{Output range} \\ \midrule
sigmoid & $\sigma(x) = \frac{1}{1+e^{-x}}$ & $\sigma'(x)=\sigma(x)(1-\sigma(x))$ & $\in 0,1$ \\
tanh & $\sigma(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$ & $\sigma'(x)= 1-\sigma(x)^{2}$ & $\in -1,1$ \\
relu & $\sigma(x) = \left\{\begin{matrix}0, x<0\\ x, x \geq 0\end{matrix}\right.$ & $\sigma'(x) = \left\{\begin{matrix}0, x<0\\ 1, x \geq 0\end{matrix}\right.$ & $\in >0,\infty$ \\ \bottomrule
\end{tabular}
\end{table}
 
More recent studies, however, looked at the limitations of FFNNs, namely the difficulty in choosing a suitable architecture and the tendency to over-fit the training data, leading to poor generalization, particularly in situations where limited labeled data is available \citep{Lu2005, Papaleonidas2013}.  

The predicted outputs in previous air quality forecast studies \citep{Arhami2013} were based on continuous concentration values measured in parts per billion (ppb) or $\mu g/m^{3}$ from single stations. Achar et al. (2011) investigated the intervals between O$_{3}$ exceedances and maxima of daily concentration levels instead of estimating real-time values. Their study of inter-occurrence between peaks was used to determine overall improvements in air quality trends over time as compared to predicting future air quality conditions \citep{Achcar2011}. 

For our validation case study area in Kuwait, several studies were completed that focused on ambient air quality and modeling using ANNs. Abdul-Wahab (2001) used 5 minute measurements of precursors CH$_{4}$, Non-Methane Hydrocarbons (NMHCs), CO, CO$_{2}$, Dust, NO, NO$_{2}$, and NO\textsubscript{x}) and meteorological inputs (WS, WD, TEMP, RH, and Solar Radiation) inputs from a mobile site in the Khaldiya residential area to estimate ozone and smog produced (SP)  using a single hidden layer FFNN \citep{AbdulWahab2001}. Al-Alawi and Abdul-Wahab later enhanced their model by applying Principal Component Analysis (PCA) to reduce the dimensionality of the input data \citep{AlAlawi2008}.  Ettouney et al. (2009) used the same inputs as Abdul-Wahab (replacing dust with Methanated Hydrocarbons) and two FFNNs to predict monthly O$_{3}$ concentrations from the Jahra and Um Al Hayman stations. They suggested that O$_{3}$ in Kuwait often comes from outside the local area via long-range transport \citep{Ettouney2009a}. 


\subsubsection{Deep Learning and Time Series}
Studies in atmospheric sciences and O$_{3}$ concentration predictions using Deep Learning (DL) have not been as common as single hidden layer FFNN. DL refers to the families of ANNs that have more than one hidden layer or use advanced architectures such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) \citep{Goodfellow2016}. 

Partially recurrent network models such as the Elman Network (EN) were used with air station inputs to predict ground level concentrations of O$_{3}$ \citep{Biancofiore2015} and PM$_{2.5}$ \citep{Biancofiore2017}. The feedback provides memory to the system when a single input set is fed into the system. Different ANN architectures are shown in Figure \ref{fig:ANNmodels}. The simple node in Figure \ref{fig:SingleANN} is redrawn as a circle for comparison.  The arrows between layers represent synaptic weights that interconnect each node. Figure \ref{fig:ANNmodels} shows that each layer can have different numbers of nodes, however, the number of nodes in deep neural networks (DNNs) hidden layers are usually kept the same for each layer.
%
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,keepaspectratio]{images/ann-models.png} 
\caption[Different ANN model architectures.]{Different ANN model architectures. (a) simple feed forward neural network, (b) a recurrent (Elman) neural network, and (c) a deep feed forward neural network with multiple hidden layers.}
\label{fig:ANNmodels}
\end{figure}
%

Implementations of procedures such as LSTM for RNNs allow network training to take place without having long-term parameters ``explode" or ``vanish" as a result of multiple learning updates \citep{Pascanu2013}. LSTM was first introduced by Hochreiter and Schmidhuber in 1997  to overcome these training issues \citep{Hochreiter1997}. Gomez (2003) was one of the first researchers to use a single layer RNN to forecast maximum ozone concentrations in Austria \citep{Gomez2003}. His model utilized LSTM to outperform other architectures such as ENs. Fan et al (2017) also used an LSTM with an RNN to forecast PM$_{2.5}$ concentrations that outperformed deep feed-forward neural networks and decision trees \citep{Fan2017}.  

DL has recently become popular for many applications due to improvements in training procedures and software libraries such as Theano \citep{Theano2016}, Tensorflow \{citep{Abadi2016}, and Keras \citep{keras2015}. These libraries have made implementing DL models more accessible, and therefore more accessible to researchers outside of the Machine Learning fields. A discussion of the theory of RNNs is presented in the next section.

\subsubsection{Time series data}
Air quality data are continuous, multivariate time series where each reading constitutes a set measurement of time and the current reading is in some way related to the previous reading, and therefore dependent \citep{Gheyas2011}. Measured pollutants may be related through photochemical or pre-cursor dependencies, while physical properties limit meteorological conditions. 

Time series are often impacted by collinearity and non-stationarity that also violate independence assumptions and can make forecast modeling difficult \citep{Gheyas2011}. Autocorrelation of individual pollutants show different degrees of dependence to past values.  Correlation coefficients were calculated using the equation
%
\begin{equation}
\label{eq:corr}
Y(\tau)= corr(X(t),X(t - \tau))
\end{equation}
%
\noindent
where X is the input vector of a time step and $\tau$ is the lag (in hours). The correlogram was plotted based on lags up to 72 hours, as shown in Figure \ref{fig:serialcorr}.
%
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,keepaspectratio]{images/time-o3.png}  %assumes jpg extension
\caption{Correlogram of O$_{3}$ and NOx for 72 hours.}
\label{fig:serialcorr}
\end{figure}
%
The parameters of Fig \ref{fig:serialcorr} show clear diurnal cycles, with O$_{3}$ having very strong relational dependence every 24 hours, regardless of the time delay. In contrast, the dependency of NOx falls rapidly over time, despite peaking every 24 hours. 

Non-stationarity, collinearity, correlations, and other linear dependencies within data are easily handled by ANNs if enough training data and hidden nodes are provided \citep{Goodfellow2016}. More important to time series are the near term history associated with the previous time step. RNNs incorporate near-term time steps by unfolding the inputs over the time sequence and sharing network weights throughout the time sequence. Additionally, the sequence fed to the RNN has fixed order, ensuring that for that individual observation, the sequence follows the order it appeared in, rather than randomly sampled as is the case for FFNN training \citep{Elangasinghe2014}. Previous models using ANNs could assume that some historical essence of the data was incorporated into the weights during updating as long as the training data was fed in temporal order and not shuffled as most categorical applications are \citep{Bengio2012}. Another way of handling sequential data is to use a time-delay neural network (TDNN). This type of architecture takes multiple time steps of data and feeds into the network at the input, using extensions of the input to represent previous states and become the system memory. TDNNs, in modern terminology, are called 1-dimensional CNNs \citep{Goodfellow2016}. They were not considered in this study.

\subsubsection{Recurrent Neural Networks}
As previously mentioned, RNNs are well suited for multivariate time series data, with the ability to capture temporal dependencies over variable periods \citep{Che2016}. RNNs have been used in many time series applications including speech recognition \citep{Graves2013}, electricity load forecasting \citep{Walid2017} and air pollution \citep{Gomez2003, Fan2017}. RNNs use the same basic building blocks as FFNNs with the addition of the output fed back into the input. This time delay feedback provides a memory feature when sequential data is fed to the RNN. The RNN share the layer's weights as the input cycles through. In Fig \ref{fig:rnn}, $X$ is the input values, $Y$ is the network output, and $H$ is the hidden layers. An FFNN is provided to compare the data flow over time. By maintaining sequential integrity, the RNN can identify long-term dependencies associated with the data, even if removed by several time steps. An RNN with one-time step, or delay, is called an Elman Network (EN) and has been used successfully to predict air quality in previous studies \citep{Biancofiore2015, Biancofiore2017}. The structure of the EN is shown in Fig \ref{fig:ANNmodels}b.

%
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,keepaspectratio]{images/rnn.png}  %assumes jpg extension
\caption{Architecture of an RNN showing layers unfolding in time n times.}
\label{fig:rnn}
\end{figure}
%

RNNs are trained using a modified version of the back-propagation algorithm called back-propagation through time (BPTT). While allowing the RNN to be trained over many different combinations of time, BPTT is vulnerable to vanishing gradients due to a large number of derivative passes, making an update very small and nearly impossible to learn correlations between remote events \citep{Pascanu2013, Graves2013a}. Different approaches were tried to resolve these training challenges including the use of gated blocks to control network weight updates such as LSTM. LSTMs will be discussed in another section. While RNNs and LSTMs have been around for many years \citep{Hochreiter1997}, their use was limited until recently, in what Goodfellow et al. calls a ``third wave of neural network research". This period began in 2006 and continues to this day \citep{Goodfellow2016}.

Like FFNN's, RNN's are trained on loss functions using optimizers to minimize the error. 

\subsubsection{Loss Functions}

The loss function, or cost function, is the function that measures the error between the predicted output and the desired output \citep{Goodfellow2016}. In optimization theory, there are many loss functions that can be used including the Mean Square Error (MSE) and cross entropy (CE) functions being the most popular for machine learning applications.  Selection of the loss function is based on the application. However, the CE is often used for classification applications \citep{Kline2005, Wu2017}. Janocha and Czarnecki (2017) suggested that non-log loss functions were more appropriate for DL based on experimental results \citep{Janocha2017}. In our study, the MSE loss function was used instead of the CE function. The MSE equation is given as 
%
\begin{equation}
\label{eq:MSE}
MSE = \frac{1}{n} \sum_{i=1}^{n} \left( y_{pred} - y_{obs} \right)^{2}
\end{equation}
%
\subsubsection{Optimizers}
Optimizers provide the method to minimize the loss function and include terms and parameters that determine the amount of incremental changes to the network weights during training. Optimizer terms often include support features such as momentum and regularization. Momentum is used to speed up convergence and avoid local minima \citep{Sutskever2013}, while regularization describes terms that reduce generalization errors \citep{Goodfellow2016}. The most common optimizer used for the back propagation training algorithm used on most neural networks is stochastic gradient descent (SGD). The basic first order SGD equations with a classic momentum (CM) term is given as
%
\begin{equation}
\label{eq:SGDterm}
g_{t+1} = \mu g_{t} - \alpha \nabla f (\theta_{t})
\end{equation}
%
\noindent
where $g$ is the gradient update term, $\mu$ is the momentum factor $(\mu \in (0,1))$, $\alpha$ is the learning rate, and $\nabla f (\theta_{t})$ is the gradient of the loss function for a specific parameter, $\theta_{t}$. The parameter is updated by
%
\begin{equation}
\label{eq:SGDupdate}
\theta_{t+1} = \theta_{t} + g_{t+1}
\end{equation}
%
A major limitation of SGD for training very deep learning networks is the vanishing gradient problem, where the gradient update term becomes so small that no update takes place and the network parameters do not converge. Hinton et al. (2006) introduced greedy layer-wise pre-training in which a network was trained layer by layer and then integrated with SGD when compiled together \citep{Hinton2006}. Since then, other first-order optimizers have been introduced that modify the SGD's basic algorithm by updating the learning rate and momentum terms during the training process \citep{Sutskever2013}.  One such method was to apply a Nesterov accelerated gradient (NAG) \citep{Nesterov1983} term to the SGD gradient update. The NAG update closely resembles the SGD update in Eq \ref{eq:SGDterm} except for the addition of another momentum term in the parameter gradient.
%
\begin{equation}
\label{eq:SGD-NAG}
g_{t+1} = \mu g_{t} - \alpha \nabla f (\theta_{t} + \mu g_{t})
\end{equation}
%
Other algorithms include the adaptive subgradient descent (AdaGrad) optimzer \citep{Duchi2011}, the root mean square propagation (RMSProp) optimizer \citep{Tieleman2012}, the adaptive momentum (Adam) optimizer \citep{Kingma2014}, and the Nesterov adaptive momentum (Nadam) optimizer \citep{Dozat2016}. A summary of how these optimizers differ is shown in Table \ref{tb-optimizers}.
%
\begin{table}[H]
\centering
\caption{Enhanced first order optimizers used for ML models.}
\label{tb-optimizers}
%\scriptsize
%\begin{tabular}{@{}lcc@{}}
\begin{tabular}{@{}p{0.2\linewidth}p{0.6\linewidth}p{0.2\linewidth}@{}}
\toprule
\textbf{Optimizer} & \textbf{Description Summary} & \textbf{Source} \\ \midrule
AdaGrad & Divides the learning rate, $\alpha$, by the $L_{2}$ norm & Duchi (2011) \\
RMSProp & Divides gradient by a running average of its recent magnitude & Tieleman (2012) \\
Adam & Combines CM with RMSProp & Kingma (2014) \\
Nadam & Combines NAG with RMSProp & Dozat (2016) \\ \bottomrule
\end{tabular}
\end{table}    

%
After experimenting with all four of the optimizers in Table \ref{tb-optimizers} and SGD, the Nadam optimizer proved to work the best with our study as described in the next section.

\subsubsection{Long Short Term Memory}

In order to preserve the memory of the data in the current state of the model, the RNN feeds parameters of its current state to the next state. This transfer can continue for multiple time steps and presented significant training challenges as mentioned earlier. The issue of vanishing gradients that took place during the BPTT updates was largely solved with the implementation of gating systems such as LSTM that allow nodes to forget or pass memory if it is not used, thus preserving enough error to allow updates \citep{Hochreiter1997}. The LSTM uses a series of gates and feedback loops that are themselves trained on the input data as shown in Fig \ref{fig:lstm}. Each node in the LSTM acts like a standard FFNN node, similar to the one in Fig \ref{eq:perceptron}, summing the inputs and applying an activation function at the output. The choice of activation function is another parameter to consider in the LSTM design. Common functions include the $sigmoid$, $tanh$, and $relu$ functions as shown in Table \ref{tb:activations}. In addition to the observations, $X$, input from the recurrent output, $Y_{R}$, representing a time-delayed element of the network, is included for a composite input of 
%
\begin{equation}
\label{eq:Xr}
X_{R}(t) = X(t) + Y_{R}(t-1)
\end{equation}
%
The processed recurrent input, $X_{R}$ feeds into several gates that allow the data to pass, represented by $\Phi$ in the circles. The weights that pass $X_{R}$ to the gate summations are trained as well.
%
\begin{figure}[H]
\centering
\includegraphics[width=.5\textwidth,keepaspectratio]{images/lstm.png} 
\caption[LSTM architecture]{LSTM architecture showing unit time delays (-1), gates and recurrent activation functions ($\sigma$).}
\label{fig:lstm}
\end{figure}
%