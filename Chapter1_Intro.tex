\chapter{Introduction}

Low ambient air quality is estimated to cause over 7 million deaths (approximately 5.4\% of all world wide deaths) in 2014. Currently, over 92\% of the world's population currently live in areas where air quality exceeds World Health Organization (WHO) limits \cite{WHO2016}. Air pollutants include primary products of combustion, products of incomplete combustion, fugitive emissions, biogenic and agricultural sources, and secondary products of combustion caused by photo-chemical transformation. The US Environmental Protection Agency (USEPA) lists 188 hazardous air pollutants (HAPs) that generators are required to report on an annual basis \cite{USEPA1996}. The main pollutants that most regulatory agencies monitor as part of their ambient air quality program is a subset of that list that includes sulfur dioxide (SO\textsubscript{2}), carbon monoxide (CO), nitrogen dioxide (NO\textsubscript{2}), tropospheric ozone (O\textsubscript{3}),  particulate matter less than 10 microns (PM\textsubscript{10}), and particulate matter less than 2.5 microns (PM\textsubscript{2.5}). Each pollutant is generated in part, as a primary or secondary process, from the thermodynamic combustion of fossil fuels.

For developing nations, heavy reliance is placed on highly polluting fuel-based combustion sources to generate electricity, power industry, transport people and products, build infrastructure, and generally improve the quality of life of its citizens. For many countries, development investment comes from exploitation of natural resources such as hydrocarbon reserves, minerals, or agricultural products - all industries that are large consumers of fossil fuels.

Countries are recognizing the cost of bad air quality on the health of their citizens and impact on world economies. The WHO estimated that over \$1.57 Trillion USD were lost due to health and mortality costs in Europe alone \cite{Roy2015} and over \$5.11 trillion USD worldwide \cite{Narain2016}.

Regulatory agencies in developing countries often face shortages in skilled personnel, funding and political will in implementing and enforcing environmental regulations and programs to protect air quality \cite{Freeman2015a}. Providing effective baseline studies and tools to establish air management programs are required that can prioritize the limited resources an agency has. 

While the major expenses in air management offices are the costs to install and maintain air monitoring stations, very little effort is given to using the data as a tool to forecast air quality levels and focus emission reduction efforts. Along with knowing what the current and historical air quality concentrations are, the most important information an air manager can have is to know where his major polluting sources are, what pollutants they generate, how much they generate of each pollutant, and when will the emissions generated by these sources create harmful concentrations.

The purpose of this research is to create novel air quality management (AQM) approaches that can support initiatives in developing nations by leveraging existing data sources. Most countries have air monitoring stations that provide ambient air quality concentrations as well as meteorological parameters. By applying historical AMS data sets with open source databases available from various government agencies, stochastic modeling, and machine learning techniques, local air managers can access advanced capabilities that would normally require expensive studies and data collection efforts.

This research approach will employ the AQM prepared for the State of Kuwait to validate and improve the novel AQM. This research will develop machine learning tools and statistical models to approximate air quality zones, prepare initial national emissions inventory, and forecast air quality concentrations.  These tools will be validated employing historical air quality monitoring data along with open access statistics from government sources.

\section{Thesis Outline}
This thesis is organized in a \textit{manuscript} format according to the University of Guelph 2016-2017 Graduate Calendar ``Thesis Format'' section (https://www.uoguelph.ca/graduatestudies/current-students/preparation-your-thesis).  The chapters are outlined as follows:

\begin{itemize}

\item \textbf{Chapter 1 - Introduction.} This chapter serves as an overview to the research covered by the thesis. It describes the work completed as well as provides a literature review of the different tasks.

\item \textbf{Chapter 2 - Problem Description.} This chapter describes the research objectives for each task and scopes the problem domain.

\item \textbf{Chapter 3 - Methodology.} This chapter provides the different methodologies developed to investigate the problem statements defined in Chapter 2.

\item \textbf{Chapter 4 - Analysis of Results and Validation.} This chapter presents results of the methods developed in Chapter 4 and compares them with other published results or expected outcomes to confirm their usefulness to addressing the problem statements of Chapter 2.

\item \textbf{Chapter 5 - Conclusions and Recommendations.} This final chapter emphasizes the overall contributions made by this research and provides recommendations for future work.

\end{itemize}
\section{Literature Review}

\subsection{Air zones review}
The use of air quality zones (AQZs) as a tool for air management was identified as early as the 1960s \citep{Breivogel1961, Holland1960} and was first mandated State Implementation Plans (SIPs) under the US Clean Air Act (CAA) of 1990 (USEPA, 1990). In the United States, AQZs are based on designated air quality control regions (AQCRs) that include defined municipalities and groups of intrastate and interstate counties (40CFR81, 1991). Currently there are 264 designated AQCRs with 121 in some form of non-compliance (or non-attainment) with the NAAQS (USEPA, 2016). In general, AQZs describe areas where air pollutants are below ambient standards (“in attainment” or “in compliance”), or exceeding them (“in non-attainment” or “in non-compliance”).  The United States Environmental Protection Agency (USEPA) uses ambient air monitoring data from state regulatory agencies measured in each county to determine non-attainment areas \citep{Carr2012}.  USEPA originally initially required states to identify zones of non-attainment for 8 hour ozone.  These regulatory requirements later included sulfur dioxide, Particulate Matter less than 10 microns (PM$_{10}$), and less than 2.5 microns (PM$_{2.5}$).  A consequence of these regulations was the installation of thousands of monitoring stations throughout the USA.  In 2014, 1,293 active ozone monitoring stations were in operation at areas considered to be at risk (USEPA, 2015).  Despite the widespread placement of monitoring equipment, designation of AQZs (or AQCRZs) are limited to areas defined by county lines \citep{Carr2012}.

The European Union initiated a zone management system for its member states in 2008 with Directive 2008/50/EC (EC, 2008).  In addition to designated AQZs, the directive requires at least one monitor for PM$_{2.5}$ in every 100,000 km$^{2}$.  Low emission zones (LEZs) were established in over 200 European cities that enforced restrictions on cars and heavy duty vehicles (Holman et al., 2015).  The drawback of LEZs is that the zones were established based on city boundaries and not by geophysical features \citep{Henschel2013}.

Turkey followed the European Directive to define AQZs for  eight different regional areas on its 81 provinces \citep{CYGM2010}.  The process used multiple methods and tools, including statistical analysis of monitoring station historical data, use of geodatabases and Geographic Information System (GIS) evaluations, air dispersion and local sources, as well as meteorological effects \citep{Karaca2012}.  Furthermore, the Turkish evaluation only employed average daily values of pollutants, instead of hourly averages, with daily values discarded if more than 30\% of hourly data was unavailable.  

China established Control Zones in 1995 that covered over 11.4\% of its territory to manage ever increasing levels of SO$_{2}$ \citep{Hao2000}.  Additional acid rain control zones covered almost 8.4\% of China.  Zones were established based on local meteorological, topographical and soil conditions that had low pH (acidic).  Other designation factors included precipitation pH values and ambient SO$_{2}$ measurements.

\subsubsection{Coastal Urban Centers}

Approximately 40\% of the population of the United States (over 100 million people) lives in coastal shoreline counties \citep{NOAA2013}.  An estimate of the world population living in coastal areas was approximately 1.4 billion people in 2015, with expected populations growing  to over 1.6 billion by 2024 \citep{Geohive2015}.  With the growth of coastal communities comes air management challenges \citep{Gamas2015}.  Local ambient air quality is impacted by the volume of emissions from industry, transportation, and residential activities, as well as meteorological and seasonal changes \citep{Fiore2015, Kimbrough2013}.  Coastal zones are also subject to land-sea breezes, caused by the diurnal differential heating/cooling of the sea and land \citep{Crosman2010, Cuxart2014, Tsai2011}.  Land-sea breezes (LSBs) continuously shift direction and speed up over the course of the day, sometimes extending as deep as 150 km inland depending on topography, and even deeper for desert regions \citep{Miao2015, Zhu2004}.  Variables that impact the extent and intensity of LSBs are largely geophysical, and include surface heat flux, wind patterns, atmospheric stability and moisture, dimensions of local water bodies, shoreline curvature, topography, and surface roughness \citep{Crosman2010, Lu1995}.  These shifting winds recirculate emissions, which have negative impacts for coastal communities \citep{Lu1996}.  Modeling and evaluating the impact of this recirculation has been the subject of several studies. \citep{Crosman2010, Levy2009, Wu2013, Zhu2004}.  A key objective of this study wais to determine the extent of LSBs along a developed coastline using a quantitative method that incorporated multi-year weather patterns. 

Statistical approaches to evaluating LSB have been applied by many researchers.  Levy et al. (2009) developed a spatio-temporal model to evaluate meso-scale recirculation using a Regional Atmospheric Modeling System HYbrid Particle and Concentration Transport (RAMS-HYPACT) modeling system and inert particles.  They used the recirculation potential index ($R$) to measure recirculation potential at each grid cell.  The $R$ index is a ratio of the wind’s vector and scalar sums over a 24 hour period.  A low numerical $R$ index indicates a constantly changing wind direction, while a high value indicates constant wind direction during the time.  Existing sources were used for model inputs and results were compared to readings from over 26 air monitoring station distributed throughout the area of evaluation.  

Wu et al. (2013) also used an $R$ index to identify LSB impacts as well as a Ventilation Index (VI) based on wind speeds at different altitudes.  Both Levy and Wu required measurements at different altitude using radio soundings or balloons to capture vertical wind speeds.  Neither team’s findings were incorporated into air zone management.  In addition to Levy and Wu, several studies effectively used neural networks and other machine learning-based techniques to analyze complex dispersion patterns in coastal areas \citep{Elangasinghe2014, Feng2015}, but these methods require large data sets for training. 

\subsubsection{Classification using higher order moments}
Classifying categories based on higher order moments, such as Skewness ($S$) and Kurtosis ($K$) statistics, were applied by Martins as early as 1965 to discriminate between beach and dune sand grains \citep{Martins1965}.  Other fields have more recently used $S-K$ statistics for pattern recognition \citep{Crosilla2013}, medical recovery \citep{Chi2008} and music genre classification (Seo and Lee, 2011).  Skewness is a 3rd order moment around the sample mean that measures the symmetry (or asymmetry) of the sample’s distribution. A normal distribution has an $S$ = 0. Kurtosis is a 4th order moment that measures the peakness of a distribution relative to a normal distribution. A normal distribution has a $K$ = 3. A highly skewed distribution with a high kurtosis will have a sharp peak and a long tail on one side of the mean \citep{NIST2013}.  The general equations for $S$ is

\begin{equation}
\label{eq:skewness}
S = \frac{\sum_{i=1}^{N}\left (x_{i}-\bar{x} \right )^{3}}{N\sigma^{3}}
\end{equation}

\noindent
and for $K$ is 

\begin{equation}
\label{eq:kurtosis}
K = \frac{\sum_{i=1}^{N}\left (x_{i}-\bar{x} \right )^{4}}{N\sigma^{4}}
\end{equation}

\noindent
where $x$ is an individual data point, $\bar{x}$ is the data mean, $\sigma$ is the standard deviation, and $N$ is the number of data points \citep{Cristelli2012}.    Some Kurtosis formulas apply a correction term of ``-3" to Eq \ref{eq:kurtosis} (called excess Kurtosis) in order to bias the equation to obtain K = 0 for a normal distribution.  Kurtosis may also be calculated as shown in eq \ref{eq:xskurtosis}.

\begin{equation}
\label{eq:xskurtosis}
K = \frac{N(N+1)}{(N-1)(N-2)(N-3)} \frac{\sum_{i=1}^{N}\left (x_{i}-\bar{x} \right )^{4}}{\sigma^{4}}
\end{equation}

The first term, $(N(N+1))/((N-1)(N-2)(N-3))$, is approximately 1/N for large values of N (N$>$200), thus reducing eq \ref{eq:xskurtosis} down to eq \ref{eq:kurtosis} \citep{Cox2010}.  Difference of distributions with various $S$ and $K$ values are shown below in Figure \ref{fig:SKcurves}.  The blue, solid line curve shows a normal distribution with $S$ = 0 and $K$ = 3. The green dot-dash line shows a Weibull distribution with $S$ = 1.7 and $K$ = 7.4.  The red dash line shows a log-normal distribution with $S$ = 4 and $K$ = 41. 
%
\begin{figure}
\includegraphics[width=\linewidth,height=22.1cm,keepaspectratio]{images/aqz1.png} 
\caption{Distributions representing various $S$ and $K$ statistics.}
\label{fig:SKcurves}
\end{figure}
%
Using the higher order moments amplifies the error of the sample points from the sample mean as well as its variance to allow better classification \citep{Seo2011}.  Crosilla et al. (2013) applied $S$ and $K$ statistics to remote sensor images in order to group terrain into homogeneous categories as based on datasets collected from Light Detection and Ranging (LiDAR).  Alberghi et al. (2002) used Kurtosis to identify vertical velocity components in convective boundary layers associated with LSBs.  They used measured data and data from air dispersion models to identify correlations between Skewness and Kurtosis \citep{Alberghi2002}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% END OF SECTION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\clearpage
\subsection{Risk characterization review}

Ambient air quality standards are specified by most countries to protect human health and reduce impacts of hazardous air pollution emissions for the public welfare. The Clean Air Act of 1970 (1970 CAA) was one of the first national legislations that established national-wide concentration levels for ambient air quality.  The original 1970 CAA National Ambient Air Quality Standards (NAAQS) included six criteria air pollutants (CAPs) with both primary (human health) and secondary (public welfare such as visibility, crops, and building material) standards \citep{USEPA1970}. A standard is assumed to include a specific air pollutant, its threshold concentration value, and the averaging time associated with the concentration value. In some cases, pollutants have multiple standards such as nitrogen dioxide (NO$_{2}$) that has a 1 hr average standard (100 parts per billion ($ppb$), 98th percentile of 1-hour daily maximum concentrations, averaged over 3 years) and an annual average standard (53 $ppb $, annual mean). The current NAAQS includes 7 air pollutant categories and 12 different standards. European air standards began with levels for sulfur dioxide and suspended particulates in 1980 \citep{EEC1980}. The World Health Organization (WHO) published recommended air quality guidelines (AGQs) as early as 1987 for Europe with ambient levels and averaging periods for 28 different chemicals and 39 standards \citep{Lubkert1994}. The current European air quality directive was adopted in 2008 and includes 12 different air pollutants with 15 different standards \citep{EU2008}, while a WHO AGQ update was published in 2005 with only five pollutants, but 25 standards \citep{WHO2006}. 

\subsubsection{Compliance Classification}

The averaging periods of each pollutant are not always clear in regard to when the measurement period begins and ends.  By convention, most air monitoring stations follow the USEPA required hourly average reporting that starts at the beginning of the hour and ends on the 59th minute \citep{CAA2007}.  A similar convention holds for 24 hour averages whereby averaging starts at midnight (00:00 hrs) to 23:59 hrs. Annual averages are based on calendar years, beginning on 1 January at 00:00 hrs and end on 31 December at 23:59 hrs.  In the case of particulate matter, reporting follows set 24 hour periods.  The USEPA’s method for taking three year averages is to average readings taken over individual quarters, average quarterly and then average the quarters over three years \citep{Cohen1999}. International standards do not have this same level of detailed description for averaging and it is the authors’ experience that USEPA methodology is assumed when guidance or interpretation is not available.

What is less clear is how results from multiple monitoring stations in one air quality zone should be aggregated. In the United States, air quality zones are based on designated air quality control regions (AQCRs) that include municipalities and groups of intrastate and interstate counties (40CFR81, 1991). Currently there are 264 designated AQCRs with 121 in some form of non-compliance (or non-attainment) with the NAAQS \citep{USEPA2016b}. 

To determine if an air quality zone is in or out of compliance with the local standards, air monitoring stations (AMSs) are used to measure ambient air and weather conditions. The USEPA uses a ``winner-take-all” approach for some pollutants such as O$_{3}$ and PM2.5 in that if one of the state and local air monitoring stations (SLAMS) within an AQCR registers exceedances that surpass a NAAQS standard, the entire zone is non-compliant \citep{USEPA2005a}.  Recently the USEPA has considered a more tailored approach to O$_{3}$ non-attainment designations by allowing regional offices to work with states and native American tribes to determine non-attainment areas based on local conditions and in some cases, declare only part of an AQCR a non-attainment area \citep{McCabe2015}. We will assume for the time being that the current ``winner takes all” approach is still in effect and used by air managers in order to demonstrate our procedure.

In the California South Coast air basin, there are at least 43 active monitoring stations \citep{CARB2013} to cover 10,743 square miles (27,824 sq km) and protect approximately 17 million people \citep{AQMD2010}.  If only one station measures an exceedance of carbon monoxide (CO) over the standard the entire basin is non-compliant and not just the immediate area.  An air monitor is assumed to uniformly represent the air quality of the area around it.  Murray and Newman (2014) recommend that lowest reading in a network area be used for multiple stations \cite{Murray2014}. This area could be as small as several hundred square meters in a micro-scale range of up to 100 meter radius from the station, to a middle range (up to 0.5 kilometer radius), neighborhood range (4 kilometer radius), urban range (50 kilometer radius) or regional range (several hundred kilometer radius) \citep{Pan2009}.  Siting air monitoring stations is, therefore, a very important process that incorporates many variables to best represent the local area \citep{Bermudez2010}, but may not represent an entire region.

In 2012, the State of Kuwait issued the Kuwait Ambient Air Quality Standards (KAAQS) in order to update their air management program as part of the Kuwait Integrated Environmental Management System (KIEMS) sponsored by the United Nations Development Program (UNDP) with the Kuwait Environment Public Authority (KEPA). The new standards included a ``winner-take-all” approach where an air zone was out of compliance for an air pollutant if any one AMS observed more than 3 observations (``3-strikes”) above the relevant standard’s threshold,  (or Guideline Value (GV),as called in the KAAQS) in a calendar year \citep{KEPA2017}. The KAAQS are shown in \ref{tb:1kaaqs}. The GVs are managed based on hourly measurements from AMSs with different averaging periods, but no daily maximums.
%
\begin{table}[!htb]
\centering
\caption{Kuwait Ambient Air Quality Standards}
\label{tb:1kaaqs}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Pollutant} & \textbf{Guideline Value} & \textbf{Averaging Time} \\ \midrule
Carbon monoxide (CO) & 35 $ppm$ & 1 hour \\
Nitrogen dioxide (NO$_{2}$) & 100 $ppb$ & 1 hour \\
 & 21 $ppb$ & Annual \\
Sulfur dioxide (SO$_{2}$) & 19 $ppb$ & 24 hours \\
 & 75 $ppb$ & 1 hour \\
Ozone (O$_{3}$) & 70 $ppb$ & 8 hours \\
Lead (Pb) & 0.15 $\mu g/m^{3}$ & Quarterly \\
Particulate Material $<10$ microns (PM$_{10}$) & 350 $\mu g/m^{3}$ & 24 hours \\
Particulate Material $<2.5$ microns (PM$_{2.5}$) & 75 $\mu g/m^{3}$ & 24 hours \\ \bottomrule
\end{tabular}
\end{table}
%
The 3-Strike method was immediately considered too conservative and ambitious for a country with a relatively new air management program and a young environmental regulatory agency. While the intent of the program was to protect human health and welfare through robust air quality standards that industry and stakeholders would strive to meet, it was also considered too stringent in that it did not take into account annual impacts of industrial output and complex land-sea breeze (LSB) weather patterns. Another compliance classification method was considered whereby 3 years of air monitoring data was collected to account for industrial output variations and seasonal weather effects. With this method, if 99\% of all hourly observations over the three years was less than or equal to the Guidance Value ($\leq$GV), then the zone was in compliance. If a zone has 2 or more AMSs, the measurements from individual stations would be pooled and the composite measurements evaluated to determine if they were $\leq$GV. This method was called the ``99\% Rule”. Using a percentile approach is similar to existing methods used by the USEPA for 1 hr SO2 and 1 hr NO$_{2}$, except the 99th percentile is used instead of the 98th \citep{USEPA2016a}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% END OF SECTION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Area sources review}

\subsubsection{Air Emissions Inventories}

Air emission inventories are used to identify air pollutants by source and process type over a given period (usually annually) and within given areas.  Many countries require an annual inventory of designated air pollutants such as the United States \citep{USEPA2014} and Kuwait \citep{KEPA2017}.  The United Nations requires signatories of the Kyoto Protocol to complete annual inventories of GHGs \citep{Paciornik2006}.  Because of the scale of emission inventories, actual annual emissions for individual sources are not available due to the lack of individual stack monitoring. In order to estimate emissions, parametric equations are developed that include factors based on the amount of fuel or feedstock consumed over the reporting period of a particular process.  An equation is developed for each pollutant from each fuel type and each process.  The quality of the calculated pollutant emission factors varies significantly depending on the degree of research applied to the fuel and process.  The most common source of published emission factors is the USEPA’s AP-42 series that includes over 27,200 different factors for pollutants from various industrial, commercial and residential processes \citep{USEPA1995}.

Different types of inventory methods exist based on the level of input data available. The Intergovernmental Panel on Climate Change (IPCC) recognizes three types of inventories.  Tier 1 inventories use IPCC published emission factors and nationally available fuel data to estimate individual pollutants.  Tier 2 uses country-specific emission factors, while Tier 3 inventories use process specific models \citep{Paciornik2006}.  The USEPA’s AP-42 only provides a factor for ammonia (NH$_{3}$) under cigarette smoking and no factors for charcoal combustion.  IPCC Tier 1 emission factors do not include emission factor specific to tobacco product use but do include factors for GHG emissions from charcoal manufacturing and usage. 

While several studies looked at indoor air quality in rooms with active nargyle pipes and smokers \citep{Fromme2009, Moon2015, Mulla2015}, specific emissions factors for nargyle smoking suitable for emission inventories were not discovered during the literature review. These studies focused on measuring chemical concentrations as compared to rates of emission. Emission rate studies on individual elements of the smoking process, such as charcoal burning and tobacco burning were identified and used to create composite factors as described in the next sections. 

\subsubsection{Emissions from $nargyla$ Pipes}
The popularity of $nargyla$ pipes throughout the world has grown in popularity over the last decade \citep{Chaouachi2009, Monzer2008}, especially among young people, where over 30\% of university students in some countries reported regular use \citep{Eissenberg2009}. This increase is a concern for public health organizations and has led to many studies looking at toxic chemical exposure from smoke generated by $nargyla$ pipes \citep{Daher2010, Eissenberg2009, Monzer2008, Sepetdjian2010, Shihadeh2005}.  Chemical analysis has shown that $nargyla$ pipe smoke has higher levels of carbon monoxide (CO), polycyclic aromatic hydrocarbons (PAHs), benzene, and other Non-Methane Organic Compounds (NMOCs) than smoking cigarettes during the same period.

The basic $nargyla$ water pipe, as shown in Fig \ref{figng1:pipe} consists of a bowl containing sweetened tobacco often referred to as $sheesha$ or $ma’saal$.  The $sheesha$ comes in many flavors and does not have the same lingering odors as other tobacco products, such as cigarettes and cigars.  The tobacco is separated by a small air gap from the glowing charcoal by aluminum foil.  The base is connected to a long tube that ends in a water filled base.  A hose attached to the base, but above the water level, allows the user to draw hot air from the charcoal onto the $sheesha$, enabling it to singe and smoke.  The smoky air is forced through the water that cools the smoke and allows the user to inhale it.  Because the $sheesha$ is not directly burned, it can be smoked for extended periods of time, sometimes over 1 hour per serving.  Studies have shown that the $sheesha$ itself contributes relatively little to the smoke other than the flavor and particulate matter with the primary source of emissions (over 95\%) coming from the combustion of the charcoal \citep{Sepetdjian2010}.

%
\begin{figure}
\includegraphics[width=\linewidth,height=10cm,keepaspectratio]{images/ng1.png} 
\caption{$Nargyla$ pipe with labeled components.}
\label{figng1:pipe}
\end{figure}
%
While most studies on $nargyla$ smoking have focused on the health effects of exposure to the toxic gases, no studies have looked at the overall contribution of smoking to greenhouse gases (GHGs) and ambient air pollution levels.  The primary contributor of air pollutants is the combustion of charcoal used to heat the $sheesha$ tobacco, which produces the GHGs carbon dioxide (CO$_{2}$), methane (CH$_{4}$), and nitrous dioxide (N$_{2}$O) in addition to hazardous air pollutants mentioned previously.  While individual pipes contribute trace amounts of pollutants during individual smoking events, this investigation will demonstrate that the combined annual impacts within a community and country are significant.
Sheesha tobacco is a mix of cut tobacco and syrups such as molasses, honey or glycerol with fruit and spice flavors \citep{Chaouachi2009}.  Unlike other tobacco products, $sheesha$ is not directly burned. Hot air from the burning charcoal provides a heat source that singes the tobacco but does not directly burn it, creating a smoke composed of particulates and gases \citep{Daher2010} that is filtered by bubbling through water.  The water removes approximately 50\% of the particulates but does not remove the gaseous components \citep{Becquemin2008}. There is a variety of $sheesha$ smoking called saloom in which the charcoal is placed directly on the tobacco. This tobacco is different from most $sheesha$ in that it is drier and does not have added flavors or syrups. The effects of $saloom$ are not considered in this study as relatively few users opt for this method of smoking.

Emissions measured in some $nargyla$ emission exposure studies used chemically processed, fast lighting coals that are not used in the Persian Gulf countries \citep{Daher2010, Shihadeh2005}.  These types of coals have accelerants mixed with the charcoal such as potassium chlorate (CAS Number 3811-04-9) \citep{MIC2012} to allow easier lighting as compared to the traditional lump charcoal that requires an external heat source to initiate combustion.  The presence of accelerants provides an additional chemical source for emissions but also reduces the overall amount of charcoal used by a caf\'e or restaurant in that a supply of charcoal must otherwise be started and re-supplied when older coals are consumed. Studies looking at different types of $nargyla$ charcoal showed that lump charcoal had more than 6 times less PAH emissions than charcoals with accelerants \citep{Sepetdjian2010}.  Only lump charcoal is used in Kuwait and therefore is the focus of this study.

Charcoal is manufactured through the pyrolysis of wood until all water, and volatile compounds are removed.  The remaining mass is often 70\% pure carbon, allowing a consistent burn rate with very little smoke. Charcoal is an ideal heat source that is often used for cooking indoors and industrial heating.  Charcoal for $nargyla$ used in the Middle East is made from lemon and orange wood or grape vines in eastern Africa.  Charcoal made from coconut husks is also common for residential use.  Manufacturing charcoal is an emissions intensive process that generates significant amounts of both HAPs and GHGs \citep{Lacaux1994}.  However, these emissions are not considered in this study.

Charcoal, once heated and glowing, burns with a surface temperature of approximately 800$^{o}$ C without additional air blowing on it \citep{Evans1977}.  Ash is formed on the surface as the combustion works its way into the fuel source. Major components of the charcoal combustion are CO$_{2}$, CO, and CH$_{4}$.  Emission factors collected from various sources are summarized in Table \ref{tb1:initialfactors}.  The factors were converted from published units (such as pounds of pollutant/ton of feedstock) to grams of pollutant/kg of feedstock (g/kg).

Only gaseous phase pollutants are quantified. Particulate matter is not considered in this study because of the assumption that the particulates stay in the room or are filtered by the ventilation system. In either case, they are assumed to not contribute to the local ambient air quality.
%
\begin{table}[]
\centering
\caption{ Emission factors in $g/kg$}
\label{tb1:initialfactors}
\begin{tabular}{@{}rcccccccccccc@{}}
\toprule
 & \multicolumn{3}{c}{\textbf{IPCC}} & \multicolumn{5}{c}{\textbf{Literature}} & \multicolumn{4}{c}{\textbf{AP-42  (USEPA, 1995)}} \\ 
 & \multicolumn{3}{c}{\textbf{Paciornik (2006)}} & \multicolumn{2}{l}{\textbf{Akagi (2011)}} & \multicolumn{2}{l}{\textbf{Bhattacharya (2002)}} & \multicolumn{1}{l}{\textbf{Sepetdijan (2010)}} & \multicolumn{1}{l}{\textbf{Mixed}} & \multicolumn{1}{l}{\textbf{Leaves}} & \multicolumn{1}{l}{\textbf{Weeds}} & \multicolumn{1}{l}{\textbf{Cigarettes}} \\
\textbf{Compound} & \textbf{Expected} & \textbf{Lower} & \textbf{Upper} & \textbf{Expected} & \textbf{variation} & \textbf{Low} & \textbf{High} & \textbf{Expected} & \textbf{Expected} & \textbf{Expected} & \textbf{Expected} & \textbf{Expected} \\ \midrule
Carbon Dioxide (CO$_{2}$) & 3304 & 1415.5 & 7656 & 2385 &  & 2155 & 2567 &  &  &  &  &  \\
Carbon Monoxide (CO) &  &  &  & 189 & 36 & 35 & 198 &  &  & 56 & 42.5 &  \\
Methane (CH$_{4}$) & 5.9 & 1.043 & 34.8 & 5.29 & 2.42 & 6.7 & 7.8 &  &  & 6 & 1.5 &  \\
Acetylene (C$_{2}$H$_{2}$) &  &  &  & 0.42 &  &  &  &  &  &  &  &  \\
Ethylene (C$_{2}$H$_{4}$) &  &  &  & 0.44 & 0.23 &  &  &  &  &  &  &  \\
Ethane (C$_{2}$H$_{6}$) &  &  &  & 0.41 & 0.13 &  &  &  &  &  &  &  \\
Methanol (CH$_{3}$OH) &  &  &  & 1.01 &  &  &  &  &  &  &  &  \\
Formaldehyde (HCHO) &  &  &  & 0.6 &  &  &  &  &  &  &  &  \\
Acetic Acid (CH$_{3}$COOH) &  &  &  & 2.62 &  &  &  &  &  &  &  &  \\
Formic Acid (HCOOH) &  &  &  & 0.063 &  &  &  &  &  &  &  &  \\
Ammonia (NH$_{3}$) &  &  &  & 0.79 &  &  &  &  &  &  &  & 0.0009 \\
Nitrogen Oxides (NOx) &  &  &  & 1.41 &  &  &  &  & 2 &  &  &  \\
Nitrous Oxide (N$_{2}$O) & 0.118 & 0.02235 & 0.87 & 0.24 &  &  &  &  &  &  &  &  \\
NMOC &  &  &  & 11.1 &  & 6 & 10 &  &  &  &  &  \\
Total PAH &  &  &  &  &  &  &  & 0.000455 & 0.0065 &  &  &  \\
Acetaldehyde &  &  &  &  &  &  &  &  & 0.545 &  &  &  \\
VOC &  &  &  &  &  &  &  &  &  & 14 & 4.5 &  \\ \bottomrule
\end{tabular}
\end{table}5	 

It is interesting to note that the IPCC factors for CO$_{2}$ are physically impossible to achieve as the maximum amount of CO$_{2}$ that could be produced from 1 kg of pure carbon is only 3,667g, but most charcoal only has a maximum total organic carbon content of 70\%, making a realistic limit of 2,567 as shown in the Bhattacharya (2002) column.  This project discovered this issue, which was acknowledged by the IPCC.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% END OF SECTION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Traffic modeling review}
In addition to greenhouse gases such as CO\textsubscript{2} and products of incomplete combustion such as CO and polycyclic aromatic hydrocarbons (PAHs), vehicles also generate wide ranges of heavy metals and trace element such as iron (Fe), aluminum (Al), copper (Cu), barium (Ba), zinc (Zn), magnesium (Mg), and titanium (Ti) \cite{Schauer2002}, \cite{Schauer2006}. 

Vehicle emissions include many more variables than stationary sources due to the wide variety of different on and off road vehicles, their ages, maintenance records, and where they are operated. A major impact on vehicle emissions is driver behavior. Aggressive driving habits, not only lead to accidents that intensify congestion, but also use more fuel for rapid accelerations and braking actions. Different variables that impact vehicle emissions are shown in Table \ref{tb:vehvariables}.  

\begin{table}[]
\centering
\caption{Variables impacting vehicle emission rates.}
\label{tb:vehvariables}
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Environment} & \textbf{Driver Behavior} & \textbf{Vehicle Condition} \\ \midrule
Road surface & Aggressiveness & Age \\
Weather conditions & Experience & Maintenance \\
Topography & Familiarity with vehicle & Fuel quality \\
Elevation &  & Gross weight \\
Traffic controls &  & Accessories \\ \bottomrule
\end{tabular}
\end{table}

Like stationary sources, vehicle emission estimates are based on emission factors developed by researchers to characterize common vehicles under specific operating conditions. Most emissions factors studies used chassis dynamometers to measure direct emission from engine exhaust but fail to capture non-exhaust emissions such as PM from brake and tire wear, vapor losses from the fuel storage systems, or leaks from lubricants and refrigerants, and hydrocarbons entrained in dust from road surfaces \citep{Kam2012, Franco2013, Freeman2015a}. While estimating emissions for individual vehicles is not practical, aggregated emissions over time can be compiled and used for regional emissions inventories and transportation planning.

\subsubsection{Vehicle emission models} \label{sssec:VehEmissionModels}

One of the most widely used model in North America is the MOtor Vehicle Emission Simulator (MOVES) from the USEPA \cite{MOVES2014a}. This model uses vehicle type, road conditions, operating activities, and geography to estimate the amount of pollutants generated in a section of road. MOVES is primarily intended for mobile sources in North America. It is a very complex and data intensive model that is difficult to apply to large, regional models \cite{Zhang2011}.

The International Vehicle Emissions (IVE) model was developed by a consortia funded by the USEPA to provide a model for vehicle emissions outside of North America \cite{IVE2008}. It includes over 700 different trypes of vehicles including 72 different classes of light duty gasoline vehicles (LGDTs) with 3 different cylinder volume subsectors. Vehicle models are further categorized by technology classes based on vehicle exhasut controls and European standards \cite{Davis2005}. The IVE model allows for customized local traffic conditions, driving patterns, fuel quality, and vehicle types for countries outside North America \cite{Davis2010}. Results from the IVE however vary with underestimation of some emissions by 50\% and over-estimation of others by 350\% \cite{Hui2007}.

\subsubsection{Classical traffic estimation} \label{sssec:ClassicalTraffic}

Each model uses a combination of composite mobile emissions based on average speed, hot/cold starts, ambient temperature, vehicle type mix and prediction year and modal factors to calculate emissions. A common requirement for each model is an understanding of the number and types of vehicles on the road, the routes and activities such as starts and stop/go cycles, as well as the speed they are traveling at \cite{Franco2013}. While the models assume constant speeds, actual driving conditions are more variable, especially in congested traffic \cite{Freeman2015b}. Vehicle emissions vary with speed with more emissions generated at higher speeds. Congestion and traffic jams, however, are the fastest growing segment of mobile source emissions, with emission rates of pollutants at slow and stop/go speeds similar to emissions at high speeds due to continuous acceleration/deceleration cycles \cite{Barth2009}.

Quantifying vehicle emissions requires, as a minimum, estimations of the number of vehicles on the road networks and the speeds they are traveling at. Macroscopic traffic flow models have been developed and modified to answer these flow and density questions for highway planning and pollution estimation. The most common model is the Lighthill-Whitham-Richards (LWR) using instantaneous traffic density $\rho$ in vehicles/km, and a function of traffic velocity, $v$ to satisfy conservation of traffic \cite{Lighthill1955}. The general equation is given as
%
\begin{equation}
\label{eq:LRW}
\frac{\partial }{\partial t}\rho \left ( t,x \right )+\frac{\partial }{\partial x}f \left ( t,x \right )=0
\end{equation}	
%
where
%
\begin{equation}
\label{eq:trafficflow}
f = \rho v
\end{equation}	
%
with units in vehicles/hr. 

The LRW model uses a fundamental diagram (FD) to express the relationship between $\rho$ and  $f$ with a maximum density, $\rho_{m}$ representing the largest amount of vehicles a road segment can support and $f_{m}$ representing the most amount of vehicles that can enter and exit the road segment. Using a linear relation proposed by Greenshields (1938), the FD looks like Figure \ref{fig:LRW-FD} \cite{Greenshields1935}. In the figure, $\rho_{O}$ is the optimum density and $f_{O}$ is the optimum flow.
%
\begin{figure}[!htbp]
\centering
\includegraphics[width=.75\textwidth]{images/traffic_curves}  %assumes jpg extension
\caption[Fundamental diagram with Greenshields's linear relationships and optimum $\rho$ and $f$]{FD with Greenshields's linear relationships and optimum $\rho$ and $f$ (Kachroo, 2014).}
\label{fig:LRW-FD}
\end{figure}
%
The LWR model assumes homogeneous traffic and very little lane changes \cite{Kachroo2014}. In developing nations, traffic fleets tend to vary dramatically and driver behavior is less disciplined \cite{Mohan2013}. Heterogeneous models such as the Aw-Rascle (AR) model, look at area occupancy for traffic concentration instead of density. The AR model has two components given as 
\begin{equation}
\label{eq:AR1}
\rho_{t}+\left (\rho v  \right )_{x}=0
\end{equation}	
%
and 
%
\begin{equation}
\label{eq:AR2}
\left [ v+p(\rho) \right ]_{t}+v\left [ \left ( v+p(\rho \right ) \right ]_{x}=\frac{V(\rho)-v}{\tau}
\end{equation}	
%
where $V(\rho)$ is an equilibrium speed for all traffic, $\tau$ is a relaxation time, and $p(\rho)$ is a traffic pressure term given as
%
\begin{equation}
\label{eq:AR-pressure}
p(\rho)=c_{o}^{2}\rho^{\gamma}
\end{equation}	
%
If the constant, $c_{o}$, is held at unity and $\gamma >0$, Eq \ref{eq:AR-pressure-reduced} reduces to 
%
\begin{equation}
\label{eq:AR-pressure-reduced}
p(\rho)=\rho^{\gamma}
\end{equation}	
%
allowing Eq \ref{eq:AR2} to be re-written as
%
\begin{equation}
\label{eq:AR2-reduced}
\left [ v+\rho^{\gamma} \right ]_{t}+v\left [v+\rho^{\gamma}\right ]_{x}=\frac{V(\rho)-v}{\tau}
\end{equation}	
%
\subsubsection{Anatomy of traffic jams}
Traffic congestion occurs when the demand for road space exceeds its capacity due to the closure of a lane or a surge of vehicles at the same time period. The resulting disruption to the equilibrium of free flowing traffic causes a reduction of speed at the front of the jam that ripples and amplifies backwards into oncoming traffic, to become stop-and-go waves of short acceleration and deceleration \citep{Orosz2010}. Modeling this action has been the subject for many researchers using linear models \citep{Lighthill1955,Treiber2000} and non-linear models \citep{Li2005}. The common features of the jam and models are:

\begin{itemize}
\item{A stimulus in the form of an initial event such as a lane closure, accident, or ''phantom" effect in which a jam begins for no obvious reason \citep{Flynn2008}.}
\item{Kinematic wave action in the form of acceleration and deceleration cycles \citep{Lighthill1955}.}
\item{Clearing action as the bottleneck is passed, resulting in a return to free flowing traffic (and usually a rapid acceleration to free-flowing speed).}
\end{itemize}

The impact of traffic jams have also been documented by many researchers in terms of impacts to health \citep{Zhang2013, Zhang2011, Levy2010} and economics \citep{Cebr2014}.  Identifying the physical locations, durations, and time periods of congestion is critical for calculating emissions, and their overall impacts to huamn health and society.

\subsubsection{Modern traffic estimation}

In many major urban centers, main roads are monitored by intelligent transportation systems (ITSs) that track average velocity, $v$, traffic flow, $f$,and traffic density, $\rho$ \cite{Wu2007}, \cite{Bartosz2015}.

In 2007, Google introduced traffic monitoring with its Google Maps Traffic Overlay (GMTO) for several US cities showing average speeds on major roads in stop-light colors to indicate free-flowing traffic (green), slow traffic (yellow or orange), congested traffic (red) and stopped traffic (dark red or black) \cite{Google2007}. Bartosiewicz and Wisniewski (2015) estimated speed ranges that were slower than Google's for urban traveling as shown in Table \ref{tb:speedcolor} \citep{Bartosiewicz2015}.
%
\begin{table}
\centering
\caption{Estimation of GMTO speed ranges.}
\label{tb:speedcolor}
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Color} & \textbf{Google estimated speeds} & \textbf{Bartosiewicz and Wisniewski (2015)} \\ \midrule
Green & $>80 kph$ & $> 80 kph$ \\
Yellow/Orange & $40 - 80 kph$ & $30 - 60 kph$ \\
Red & $<40 kph$ & $15 - 30 kph$ \\
Dark Red/Black & $0 - 20 kph$ (stop and go) & $0 - 15 kph$ \\ \bottomrule
\end{tabular}
\end{table}
%
This traffic mapping feature is currently available in most cities around the world with an example of different conditions in Kuwait City shown in Figure \ref{fig:GoogleTraffic}. The high congestion shown 1 December is common for urban areas during rush hour before the weekend \footnote{In Kuwait the work week runs from Sunday to Thursday with weekend days on Friday and Saturday.} while the free flowing traffic on 2 December is typical for the weekend.
%
\begin{figure}[!htbp]
\centering
\includegraphics[width=.75\textwidth]{images/googletraffic.png}  
\caption[Snapshot of traffic in Google Maps]{Snapshot of traffic in Kuwait City from Google Maps on 1 Dec and 2 Dec 2016.}
\label{fig:GoogleTraffic}
\end{figure}
%
Google uses anonymous tracking data from mobile phones provided from cellular service providers as they move between cell towers, creating a crowd-sourced representation of traffic conditions. Multiple sources are needed to provide an average speed as well as mask individual vehicle movements \cite{Barth2009a}.  With this type of \textit{ad hoc} traffic monitoring already in place in most countries and urban areas, drivers and traffic planners have almost real time access to wide area traffic conditions. This information has been used with other ITS inputs, to assist traffic managers\citep{Wu2007}. Colak et al. (2016) used call detail records (CDRs) from mobile phones, similar to Google's approach, to capture travel profiles in order to evaluate travel times over distance traveled \citep{Colak2016}.  While these methods use crowd-source data, they were not used to provide vehicle density estimates that could be used for emission inventories.\\

Without using expensive ITS's, different numerical models have been used to estimate vehicle density, but without incorporating the additional capabilities GMTO can bring in terms of near real time updates. These include classical statisical models \citep{Schreckenberg1995}, Kalman Filters \citep{Pourmoallem1997, Sun2004} and neural networks \citep{Ghosh-Dastidar2006}.  These models looked at how traffic flowed over time to assess traffic management strategies and required complex computations and historical data to calibrate the necessary equations for a specific stretch of road.  Monte Carlo methods have also been used to validate results of traffic flow models \citep{Mihaylova2004} but not to generate results.  These models look at traffic flow under various conditions and not at the extreme condition of congested traffic.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% END OF SECTION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Concentration prediction review}

Tropospheric, or surface ozone (O$_{3}$)is a secondary pollutant formed by complex photo-chemical processes that impacts human health, plants, and structural materials. Over 21,000 premature deaths in Europe are attributed annually to O$_{3}$ exposure \citep{WHO2008} with over 1.1 million deaths worldwide - over 20\% of all deaths attributed to respiratory diseases \citep{Malley2017}. The majority of tropospheric O$_{3}$ is generated through anthropogenic sources \citep{Lelieveld2000, Cooper2006} attributed to the photo-disassociation of NO$_{2}$ as shown below in the simplified reaction \citep{Finlayson1993}:

\begin{equation}
\label{eq:ozoneformation}
\begin{gathered}
NO_{2}+h\nu (\lambda < 430nm) \rightarrow NO+O \\
O+O_{2}\overset{M}{\rightarrow} O_{3}
\end{gathered}
\end{equation}

\noindent
where $M$ is a stabilization molecule used during the intermediate formation between O and O$_{2}$. Volatile Organic Compounds (VOCs) are not shown in Eq \ref{eq:ozoneformation}, but play a significant role in the oxidation of the primary combustion product NO to NO$_{2}$ \citep{Song2011}. In addition to nitrogen oxides (NOx's), VOCs, chlorine \citep{Thornton2010}, solar radiation (SR), relative humidity (RH) and ambient temperature also impact surface ozone formation \citep{Sadanaga2003}.  Local concentrations of O$_{3}$ are further influenced by weather patterns and terrain that disperse the pollutants, precursors, and byproducts \citep{Beck1998}. At night, O$_{3}$ reacts with NO$_{2}$ to form NO$_{3}$ (nitrate radical) \citep{Finlayson1993}:

\begin{equation}
\label{eq:nitrateformation}
O_{3} + NO_{2}\rightarrow NO_{3}+O_{2} 
\end{equation}

The NO$_{3}$ radicals react with NO$_{2}$ to form dinitrogen pentoxide (N$_{2}$O$_{5}$) which in turn forms nitric acid (HNO$_{3}$) through hydrolysis with water or aqueous particles \citep{Song2011}. The acid is finally neutralized by ammonia (NH$_{3}$) to complete the reaction chain \citep{Brown2012}.

Additional contributions to tropospheric O$_{3}$ concentrations come from the stratosphere-troposphere exchange (STE) of stratospheric ozone \citep{Tarasick2008}. The percentage of O$_{3}$ provided by STE at surface levels range from 13\% \citep{Cooper2006} to over 42\% \citep{Lelieveld2000} depending on area and conditions. With so many chemical and transport phenomena taking place throughout the day and night, modeling O$_{3}$ becomes a very complex task even before local terrain, sources and weather patterns are incorporated. Nonetheless, predicting ambient O$_{3}$ concentrations, and particularly concentrations that may exceed air quality standards, is important for air managers and at-risk populations.  In cases where O$_{3}$ levels will exceed standards for long periods of time, air managers may issue air quality warnings and even limit industrial and vehicular activities \citep{Kuhlbusch2014, Welch2005}. Improving forecast accuracy provides planning and decision options that can impact receptor health and local economies.

\subsubsection{Forecasting Ozone with Machine Learning}

Due to the formation process of O$_{3}$, the actual concentration a local population is exposed to may have been generated from precursors emitted hundreds or even thousands of kilometers away \citep{Glavas2011}. Populations living in coastal regions may be exposed to pollutants generated locally but transformed and mixed with other precursors in circulating land-sea breezes \citep{Freeman2017a}. Surface O$_{3}$ is therefore a more complex pollutant to estimate than primary pollutants such as sulfur dioxide (SO$_{2}$) or carbon monoxide (CO).

Many studies have used supervised machine learning techniques, such as artificial neural networks (ANNs) to predict O$_{3}$ time series concentrations \citep{Comrie1997, Dorling2003, Ettouney2009a, Kurt2008, Biancofiore2017}. The benefits of using ANNs include not requiring \textit{a priori} assumptions of the data used for training and not requiring weighting of initial inputs \citep{Gardner1998}. In practice, dimensionality reduction is often used to remove inputs to the model that are not independent and identically distributed (IID) or offer little influence to the overall training. Principal Component Analysis (PCA) is often used to reduce the overall inputs to the model by removing transformed components, but provide little variability to the actual raw observations required to train \citep{Singh2013, Wang2015a}.

Because of the complex chemistry of O$_{3}$ formation and local concentration patterns based on weather conditions, ANNs have been shown to provide better predictive results than linear models such as Multiple Linear Regression (MLR) and time series modeling such as Autoregressive Integrated Moving Averages (ARIMA) \citep{Gardner1998, Prybutok2000}. 

Most of the studies that use ANNs apply a single hidden-layer feed forward neural network architecture trained with meteorological and concentration data and generally have limited success for forecasting air quality. The canonical feed forward ANN model (FFNN) consists of an input layer, a hidden layer and an output layer. Each layer is constructed from interlinked nodes that generates a value (usually between -1 and 1 or 0 and 1). The individual node model is shown in Fig \ref{fig:SingleANN}. \\
%
\begin{figure}
\centering
\includegraphics[width=.75\textwidth]{images/single-ann.png} 
\caption{Individual node model.}
\label{fig:SingleANN}
\end{figure}
%
The node sums the weighted inputs of the previous layer, sometimes with a bias, and transforms the combined sum with a non-linear activation function, $\sigma$. The node activation equation is given by

\begin{equation}
\label{eq:perceptron}
y= \sigma(wx+b)
\end{equation}

\noindent
where $w$ is an array of weights for the connections between the previous layer and the current layer, $x$ is a vector of input values from the previous layer, and $b$ is a bias value. Common activation functions include the sigmoid, tanh, and relu functions. A general property for activation functions is that they normalize the output and have a continuous first order derivative that can be used during the back propagation training process \citep{Goodfellow2016}. The common activation functions mentioned earlier are shown in Table \ref{tb:activations} along with their first order derivative and output range.

\begin{table}[]
\centering
\caption{Common activation functions}
\label{tb:activations}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Name} & \textbf{Equation} & \textbf{Derivative} & \textbf{Output range} \\ \midrule
sigmoid & $\sigma(x) = \frac{1}{1+e^{-x}}$ & $\sigma'(x)=\sigma(x)(1-\sigma(x))$ & $\in 0,1$ \\
tanh & $\sigma(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$ & $\sigma'(x)= 1-\sigma(x)^{2}$ & $\in -1,1$ \\
relu & $\sigma(x) = \left\{\begin{matrix}0, x<0\\ x, x \geq 0\end{matrix}\right.$ & $\sigma'(x) = \left\{\begin{matrix}0, x<0\\ 1, x \geq 0\end{matrix}\right.$ & $\in >0,\infty$ \\ \bottomrule
\end{tabular}
\end{table}
 
More recent studies, however, looked at the limitations of feed forward networks, namely the difficulty in choosing a suitable architecture and the tendency to over-fit the training data, leading to poor generalization, particularly in situations where limited labeled data is available \citep{Lu2005, Papaleonidas2013}.  

The predicted outputs in previous air quality forecast studies \citep{Arhami2013} were based on continuous concentration values measured in parts per million (ppm) or $\mu g/m^{3}$ from single stations. Achar et al. (2011) investigated the intervals between O$_{3}$ exceedances and maxima of daily concentration levels instead of estimating real time values. Their study of inter-occurrence between peaks was used to determine improvements to air quality over time as compared to predicting future conditions \citep{Achcar2011}. 

For out validation case study area in Kuwait, several studies were completed that focused on ambient air quality and modeling using ANNs. Abdul-Wahab (2001) used 5 minute measurements of precursors CH\textsubscript{4}, Non-Methane Hydrocarbons (NMHCs), CO, CO\textsubscript{2}, Dust, NO, NO\textsubscript{2}, and NO\textsubscript{x}) and meteorological (WS, WD, TEMP, RH, and Solar Radiation) inputs from a mobile site in the Khaldiya residential area to estimate ozone and smog produced (SP)  using a single hidden layer FF ANN \citep{AbdulWahab2001}. Al-Alawi and Abdul-Wahab later enhanced their model by applying Principal Component Analysis (PCA) to reduce the dimensionality of the input data \citep{AlAlawi2008}.  Ettouney et al. (2009) used the same inputs as Abdul-Wahab (replacing dust with Methanated Hydrocarbons) and two FF networks to predict monthly O$_{3}$ concentrations from the Jahra and Um Al Hayman stations. They suggested that O$_{3}$ in Kuwait often comes from outside the local area via long range transport \citep{Ettouney2009a}. 

Other machine learning techniques applied to ozone and air quality predictions include the use of support vector machines (SVMs) \citep{Luna2014, Papaleonidas2013, Singh2013} and logistic regression \citep{Zickus2002}. 

\subsubsection{Deep Learning and Time Series}
Studies in atmospheric sciences and O$_{3}$ concentration predictions using Deep Learning (DL) have not been as common as single hidden layer ANN. DL refer to the families of ANNs that have more than one hidden layer or use advanced architectures such as recurrent neural networks (RNNs) or convolutional neural networks (CNNs) \citep{Goodfellow2016}. 

RNNs are particularly well suited for air concentration prediction because they incorporate sequential history into the training and processing of input data. Air quality measurements are time series datasets in which the order of data is important. Previous models using ANNs could assume that some historical essence of the data was incorporated into the weights during updating as long as the training data was fed in temporal order and not shuffled as most categorical applications are \citep{Bengio2012}.

Partially recurrent network models such as the Elman Network (EN) has been used  with air station inputs to predict ground level concentrations of O$_{3}$ \citep{Biancofiore2015} and PM2.5 \citep{Biancofiore2017}. The feedback provides memory to the system when a single input set is fed into the system. Different ANN architectures are shown in Figure \ref{fig:ANNmodels}. The simple network in Figure \ref{fig:SingleANN} has been redrawn for comparison.  The arrows between layers represent synaptic weights that interconnect each node.
%
\begin{figure}
\centering
\includegraphics[width=.75\textwidth]{images/ann-models.png} 
\caption{Different ANN model architectures.}
\label{fig:ANNmodels}
\end{figure}
%
Figure \ref{fig:ANNmodels} shows that each layer can have different numbers of nodes, however, the number of nodes in the DNN hidden layers are usually kept the same for each layer. 

Another way of handling sequential data is to use a time-delay neural network (TDNN). This type of architecture takes multiple time steps of data and feeds into the network at the input, using extensions of the input to represent previous states and become the system memory. TDNNs, in modern terminology, are called 1-dimensional CNNs \citep{Goodfellow2016}. They were not considered in this study.

Implementations of procedures such as long short-term memory (LSTM) for RNNs allow network training to take place without having long term parameters ``explode" or ``vanish" as a result of multiple learning updates \citep{Pascanu2013}. LSTM was first introduced by Hochreiter and Schmidhuber in 1997 as a means to overcome these training issues \citep{Hochreiter1997}. Gomez (2003) was one of the first researchers to use a single layer RNN to forecast maximum ozone concentrations in Austria \citep{Gomez2003}. His model utilized LSTM to outperform other architectures such as ENs. Noting the gap of years from Gomez et al. in 2003 and the work by Biancofiore et al. as recently as 2017 using ENs show that the complexity of preparing and training RNNs that use LSTM has kept researchers from using DL methods.

DL has recently become popular for many applications due to improvements in training procedures and software libraries in Python such as Theano \citep{Theano2016} and Keras \citep{keras2015}. These libraries have made implementing DL models easier, and therefore more accessible for researchers outside of the Machine Learning fields. A discussion of the theory of RNNs is presented in the next section.

\subsubsection{Time series data}
Air quality data are continuous, multi-variate time series where each reading constitutes a set measurement of time and the current reading is in some way related to the previous reading, and therefore dependent \citep{Gheyas2011}. Measured pollutants may be related through photochemical or pre-cursor dependencies, while meteorological conditions are limited by physical properties. 

Time series are often impacted by collinearity and non-stationarity that also violate independence assumptions and can make forecast modeling difficult \citep{Gheyas2011}. Autocorrelation of individual pollutants show different degrees of dependence to past values.  Correlation coefficients were calculated using the equation
%
\begin{equation}
\label{eq:corr}
Y(\tau)= corr(X(t),X(t - \tau))
\end{equation}
%
\noindent
where X is the input vector of a time step and $\tau$ is the lag (in hours). The correlogram was plotted based on lags up to 72 hours, as shown in Figure \ref{fig:serialcorr}.
%
\begin{figure}
\centering
\includegraphics[width=.75\textwidth]{images/time-o3.png}  %assumes jpg extension
\caption{Correlogram of O$_{3}$ and NOx for 72 hours.}
\label{fig:serialcorr}
\end{figure}
%
The parameters of Fig \ref{fig:serialcorr} show clear diurnal cycles, with O$_{3}$ having very strong relational dependence every 24 hours, regardless of the time delay. In contrast, the dependency of NOx falls rapidly over time, despite peaking every 24 hours. 

Non-stationarity, collinearity, correlations, and other linear dependencies within data are easily handled by ANNs if enough training data and hidden nodes are provided \citep{Goodfellow2016}. More important to time series are the near term history associated with the previous time step. Recurrent neural networks (RNNs) incorporate near term time steps by unfolding the inputs over the time sequence and sharing network weights throughout the time sequence. Additionally, the sequence fed to the RNN has fixed order, ensuring that for that individual observation, the sequence follows the order it appeared in, rather than randomly sampled as is the case for feed forward network training \citep{Elangasinghe2014}.

\subsubsection{Recurrent Neural Networks}
Recurrent neural networks (RNNs) are well suited for multivariate time series data, with the ability to capture temporal dependencies over variable terms \citep{Che2016}. RNNs have been used in many time series applications including speech recognition \citep{Graves2013}, electricity load forecasting \citep{Walid2017} and air pollution \citep{Gomez2003}. RNNs use the same basic building blocks as FFNNs with the addition of the output fed back into the input. This time delay feedback provides a memory feature when sequential data is fed to the RNN. The RNN share the layer's weights as the input cycles through. In Fig \ref{fig:rnn}, $X$ is the input values, $Y$ is the network output, and $H$ is the hidden layers. A feed forward network is provided to compare the data flow over time. By maintaining sequential integrity, the RNN can identify long-term dependencies associated with the data, even if removed by several time steps. An RNN with one time step, or delay, is called an Elman Network (EN) and has been used successfully to predict air quality in previous studies \citep{Biancofiore2015, Biancofiore2017}. The structure of the EN was shown in Fig \ref{fig:ANNmodels}b.

%
\begin{figure}
\centering
\includegraphics[width=.75\textwidth]{images/rnn.png}  %assumes jpg extension
\caption{Architecture of an RNN showing layers unfolding in time n times.}
\label{fig:rnn}
\end{figure}
%

RNNs are trained using a modified version of the back propagation algorithm called back propagation through time (BPTT). While allowing the RNN to be trained over many different combinations of time, BPTT is vulnerable to vanishing gradients due to a large number of derivative passes, making an update very small and nearly impossible to learn correlations between remote events \citep{Pascanu2013, Graves2013a}. Different approaches were tried to resolve these training challenges including the use of gated blocks to control network weight updates such as long short term memory (LSTM). LSTMs will be discussed in another section. While RNNs and LSTMs have been around for many years \cite{Hochreiter1997}, their use was limited until recently, in what Goodfellow et al. calls a ``third wave of neural network research". This period began in 2006 and continues to this day \citep{Goodfellow2016}.

Like FFNN's, RNN's are trained on loss functions using optimizers to minimize the error. A brief discussion of these two parameters is provided below.

\subsubsection{Loss Functions}

The loss function, or cost function, is the function that measures the error between the predicted output and the desired output \citep{Goodfellow2016}. In optimization theory, there are many loss functions that can be used including the Mean Square Error (MSE) and cross entropy (CE) functions being the most popular for machine learning applications.  Selection of the loss function is based on the application. However, the CE is often used for classification applications \citep{Kline2005, Wu2017}. Janocha and Czarnecki (2017) suggested that non-log loss functions were more appropriate for DL based on experimental results \citep{Janocha2017}. In our study, the MSE loss function was used instead of the CE function. The MSE equation is given as 
%
\begin{equation}
\label{eq:MSE}
MSE = \frac{1}{n} \sum_{i=1}^{n} \left( y_{pred} - y_{obs} \right)^{2}
\end{equation}
%
\subsubsection{Optimizers}
Optimizers provide the method to minimize the loss function. Parameters include a learning rate that determines the amount of incremental change to the network weights as well as learning to support features such as momentum and regularization. The most common optimizer used for the back propagation training algorithm used on most neural networks is stochastic gradient descent (SGD). The basic first order SGD equations with a classic momentum (CM) term is given as
%
\begin{equation}
\label{eq:SGDterm}
g_{t+1} = \mu g_{t} - \alpha \nabla f (\theta_{t})
\end{equation}
%
\noindent
where $g$ is the gradient update term, $\mu$ is the momentum factor $(\mu \in (0,1))$, $\alpha$ is the learning rate, and $\nabla f (\theta_{t})$ is the gradient of the loss function for a specific parameter, $\theta_{t}$. The parameter is updated by
%
\begin{equation}
\label{eq:SGDupdate}
\theta_{t+1} = \theta_{t} + g_{t+1}
\end{equation}
%
A major limitation of SGD for training very deep learning networks is the vanishing gradient problem, where the gradient update term becomes some small that no update takes place and the network parameters do not converge. Hinton et al. (2006) introduced greedy layerwise pre-training in which a network was trained layer by layer and then integrated with SGD when compiled together \citep{Hinton2006}. Since then, other first-order optimizers have been introduced that modify the SGD's basic algorithm by updating the learning rate and momentum terms during the training process \citep{Sutskever2013}.  One such method was to apply a Nesterov accelerated gradient (NAG) \citep{Nesterov1983} term to the SGD gradient update. The NAG update closely resembles the SGD update in Eq \ref{eq:SGDterm} except for the addition of another momentum term in the parameter gradient.
%
\begin{equation}
\label{eq:SGD-NAG}
g_{t+1} = \mu g_{t} - \alpha \nabla f (\theta_{t} + \mu g_{t})
\end{equation}
%
Other algorithms include the adaptive subgradient descent (AdaGrad) optimzer \citep{Duchi2011}, the root mean square propagation (RMSProp) optimizer \citep{Tieleman2012}, the adaptive momementum (Adam) optimizer \citep{Kingma2014}, and the Nesterov adaptive momentum (Nadam) optimizer \citep{Dozat2016}. A summary of how these optimizers differ is shown in Table \ref{tb-optimizers}.
%
\end{linenumbers}
\begin{table}[]
\centering
\caption{Enhanced first order optimizers used for DLNs}
\label{tb-optimizers}
\scriptsize
\begin{tabular}{@{}lcc@{}}
%\begin{tabular}{@{}p{2cm}p{8cm}p{2.5cm}@{}}
\toprule
\textbf{Optimizer} & \textbf{Description Summary} & \textbf{Source} \\ \midrule
AdaGrad & Divides the learning rate, $\alpha$, by the $L_{2}$ norm & Duchi (2011) \\
RMSProp & Divides gradient by a running average of its recent magnitude & Tieleman (2012) \\
Adam & Combines CM with RMSProp & Kingma (2014) \\
Nadam & Combines NAG with RMSProp & Dozat (2016) \\ \bottomrule
\end{tabular}
\end{table}	
\begin{linenumbers}
%
After experimenting with all four of the optimizers in Table \ref{tb-optimizers} and SGD, the Nadam optimizer proved to work the best with our study as described in the next section.

\subsubsection{Long Short Term Memory}

In order to preserve the memory of the data in the current state of the model, the RNN feeds parameters of its current state to the next state. This transfer can continue on for multiple time steps and presented significant training challenges as mentioned earlier. The issue of vanishing gradients that took place during the BPTT updates was largely solved with the implementation of gating systems such as long short term memory (LSTM) that allow nodes to forget or pass memory if it is not being used, thus preserving enough error to allow updates \citep{Hochreiter1997}. The LSTM uses a series of gates and feedback loops that are themselves trained on the input data as shown in Fig \ref{fig:lstm}. Each individual node acts like a perceptron, similar to the one in Fig \ref{eq:perceptron}, summing the inputs and applying an activation function at the output. The choice of activation function is another parameter to consider in the LSTM design. Common functions include the $sigmoid$, $tanh$, and $relu$ functions as shown in Table \ref{tb:activations}. The difference to the node input is that in addition to the observation data $X$, additional input from the recurrent output, $Y_{R}$, representing a time delayed element of the network, is included for a composite input of 
%
\begin{equation}
\label{eq:Xr}
X_{R}(t) = X(t) + Y_{R}(t-1)
\end{equation}
%
The processed recurrent input, $X_{R}$ feeds into several gates that allow the data to pass, represented by $\Phi$ in the circles. The weights that pass $X_{R}$ to the gate summations are trained as well.
%
\begin{figure}
\centering
\includegraphics[width=.75\textwidth]{images/lstm.png} 
\caption[LSTM architecture]{LSTM architecture showing unit time delays (-1), gates and recurrent activation functions ($\sigma$).}
\label{fig:lstm}
\end{figure}
%
The use of LSTM in RNN architecture allows long term dependencies in data to be remembered within the model \citep{Graves2013a}, a feature required when working with sequential series,  such as air pollution concentration over time.
